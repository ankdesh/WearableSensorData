{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.classification import accuracy_score, recall_score, f1_score\n",
    "import scipy.stats as st\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../quantnn-keras/layers/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.losses import squared_hinge\n",
    "from keras.layers import Flatten, Dense, Activation, BatchNormalization, Conv1D\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "import numpy as np\n",
    "\n",
    "from binary_layers import BinaryConv2D, BinaryConv1D\n",
    "from binary_ops import binary_tanh as binary_tanh_op\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet Template 2017 data/LOSO/MHEALTH.npz\n",
      "255/255 [==============================] - 0s 413us/step\n",
      "[0.9179191189653734, 0.7960784323075238]\n",
      "263/263 [==============================] - 0s 433us/step\n",
      "[0.921292200061305, 0.5779467682874702]\n",
      "251/251 [==============================] - 0s 461us/step\n",
      "[0.9293723239366751, 0.44223107522227373]\n",
      "266/266 [==============================] - 0s 332us/step\n",
      "[0.9186151646133652, 0.6315789478165763]\n",
      "265/265 [==============================] - 0s 460us/step\n",
      "[0.9184935306603054, 0.6867924528301886]\n",
      "264/264 [==============================] - 0s 472us/step\n",
      "[0.9182958512595205, 0.6856060606060606]\n",
      "253/253 [==============================] - 0s 428us/step\n",
      "[0.9245998352883833, 0.36363636304738495]\n",
      "239/239 [==============================] - 0s 371us/step\n",
      "[0.9211378686098873, 0.6861924688062907]\n",
      "251/251 [==============================] - 0s 437us/step\n",
      "[0.9332977623578562, 0.609561756075141]\n",
      "248/248 [==============================] - 0s 417us/step\n",
      "[0.9205247375272936, 0.600806450651538]\n"
     ]
    }
   ],
   "source": [
    "data_input_file = 'data/LOSO/WISDM.npz'\n",
    "\n",
    "tmp = np.load(data_input_file)\n",
    "X = tmp['X']\n",
    "y = tmp['y']\n",
    "folds = tmp['folds']\n",
    "\n",
    "n_class = y.shape[1]\n",
    "\n",
    "avg_acc = []\n",
    "avg_recall = []\n",
    "avg_f1 = []\n",
    "y = np.argmax(y, axis=1)\n",
    "\n",
    "print('ConvNet Template 2017 {}'.format(data_input_file))\n",
    "\n",
    "for i in range(0, len(folds)):\n",
    "    train_idx = folds[i][0]\n",
    "    test_idx = folds[i][1]\n",
    "\n",
    "    X_train, y_train = X[train_idx], y[train_idx]\n",
    "    X_test, y_test = X[test_idx], y[test_idx]\n",
    "    X_train = X_train.squeeze()\n",
    "    X_test = X_test.squeeze()\n",
    "    y_train = to_categorical(y_train)\n",
    "    y_test = to_categorical(y_test)\n",
    "    \n",
    "    # Create Model\n",
    "    model = Sequential()\n",
    "\n",
    "    _momentum = 0.1\n",
    "    _epsilon = 0.001\n",
    "    \n",
    "    model.add(\n",
    "        BinaryConv1D(\n",
    "            kernel_size=30,\n",
    "            filters=64,\n",
    "            padding='same',\n",
    "            input_shape=(250,23),\n",
    "            kernel_lr_multiplier=10,use_bias=False))\n",
    "    model.add(BatchNormalization(momentum=_momentum, epsilon=_epsilon))\n",
    "    model.add(Activation(binary_tanh_op))\n",
    "\n",
    "\n",
    "    model.add(\n",
    "        BinaryConv1D(\n",
    "            kernel_size=30, filters=128, padding='same',\n",
    "            kernel_lr_multiplier=10,use_bias=False))\n",
    "    model.add(BatchNormalization(momentum=_momentum, epsilon=_epsilon))\n",
    "    model.add(Activation(binary_tanh_op))\n",
    "\n",
    "\n",
    "    model.add(\n",
    "        BinaryConv1D(\n",
    "            kernel_size=30, filters=256, padding='same',\n",
    "            kernel_lr_multiplier=10,use_bias=False))\n",
    "    model.add(BatchNormalization(momentum=_momentum, epsilon=_epsilon))\n",
    "    model.add(Activation(binary_tanh_op))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(12, use_bias=False))\n",
    "    model.add(BatchNormalization(momentum=_momentum, epsilon=_epsilon))\n",
    "\n",
    "    # Train\n",
    "    adam = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    model.compile(loss=squared_hinge, optimizer=adam, metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "\n",
    "    model.fit(\n",
    "        X_train, y_train, batch_size=50, epochs=100, validation_data=(X_test,y_test), verbose=0 )\n",
    "    print (model.evaluate(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet Template 2017 data/LOSO/WISDM.npz\n",
      "(20167, 100, 3) (679, 100, 3)\n",
      "(20167, 6) (679, 6)\n"
     ]
    }
   ],
   "source": [
    "data_input_file = 'data/LOSO/WISDM.npz'\n",
    "\n",
    "tmp = np.load(data_input_file)\n",
    "X = tmp['X']\n",
    "y = tmp['y']\n",
    "folds = tmp['folds']\n",
    "\n",
    "n_class = y.shape[1]\n",
    "\n",
    "avg_acc = []\n",
    "avg_recall = []\n",
    "avg_f1 = []\n",
    "y = np.argmax(y, axis=1)\n",
    "\n",
    "print('ConvNet Template 2017 {}'.format(data_input_file))\n",
    "\n",
    "i = 0\n",
    "train_idx = folds[i][0]\n",
    "test_idx = folds[i][1]\n",
    "\n",
    "X_train, y_train = X[train_idx], y[train_idx]\n",
    "X_test, y_test = X[test_idx], y[test_idx]\n",
    "X_train = X_train.squeeze()\n",
    "X_test = X_test.squeeze()\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "print (X_train.shape, X_test.shape)\n",
    "print (y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36, 2) 36\n"
     ]
    }
   ],
   "source": [
    "print (folds.shape, len(folds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet Template 2017 data/LOSO/WISDM.npz\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "binary_conv1d_4 (BinaryConv1 (None, 100, 64)           1920      \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 100, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 100, 64)           0         \n",
      "_________________________________________________________________\n",
      "binary_conv1d_5 (BinaryConv1 (None, 100, 128)          81920     \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 100, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 100, 128)          0         \n",
      "_________________________________________________________________\n",
      "binary_conv1d_6 (BinaryConv1 (None, 100, 128)          163840    \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 100, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 100, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 12800)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 76800     \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 6)                 24        \n",
      "=================================================================\n",
      "Total params: 325,784\n",
      "Trainable params: 325,132\n",
      "Non-trainable params: 652\n",
      "_________________________________________________________________\n",
      "Train on 20167 samples, validate on 679 samples\n",
      "Epoch 1/100\n",
      "20167/20167 [==============================] - 36s 2ms/step - loss: 0.9476 - acc: 0.3383 - val_loss: 0.8785 - val_acc: 0.4006\n",
      "Epoch 2/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.9023 - acc: 0.3911 - val_loss: 0.8713 - val_acc: 0.5125\n",
      "Epoch 3/100\n",
      "20167/20167 [==============================] - 34s 2ms/step - loss: 0.8841 - acc: 0.4516 - val_loss: 0.8807 - val_acc: 0.5184\n",
      "Epoch 4/100\n",
      "20167/20167 [==============================] - 34s 2ms/step - loss: 0.8710 - acc: 0.5019 - val_loss: 0.8705 - val_acc: 0.5479\n",
      "Epoch 5/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8633 - acc: 0.5450 - val_loss: 0.8610 - val_acc: 0.5287\n",
      "Epoch 6/100\n",
      "20167/20167 [==============================] - 34s 2ms/step - loss: 0.8578 - acc: 0.5794 - val_loss: 0.8543 - val_acc: 0.6141\n",
      "Epoch 7/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8542 - acc: 0.6043 - val_loss: 0.8514 - val_acc: 0.6981\n",
      "Epoch 8/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8509 - acc: 0.6225 - val_loss: 0.8502 - val_acc: 0.7113\n",
      "Epoch 9/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8484 - acc: 0.6442 - val_loss: 0.8609 - val_acc: 0.6745\n",
      "Epoch 10/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8461 - acc: 0.6649 - val_loss: 0.8487 - val_acc: 0.7997\n",
      "Epoch 11/100\n",
      "20167/20167 [==============================] - 36s 2ms/step - loss: 0.8446 - acc: 0.6720 - val_loss: 0.8531 - val_acc: 0.7776\n",
      "Epoch 12/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8426 - acc: 0.6932 - val_loss: 0.8531 - val_acc: 0.7261\n",
      "Epoch 13/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8415 - acc: 0.7096 - val_loss: 0.8533 - val_acc: 0.7673\n",
      "Epoch 14/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8403 - acc: 0.7212 - val_loss: 0.8454 - val_acc: 0.7938\n",
      "Epoch 15/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8394 - acc: 0.7285 - val_loss: 0.8585 - val_acc: 0.7938\n",
      "Epoch 16/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8390 - acc: 0.7392 - val_loss: 0.8647 - val_acc: 0.7850\n",
      "Epoch 17/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8388 - acc: 0.7347 - val_loss: 0.8513 - val_acc: 0.8041\n",
      "Epoch 18/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8379 - acc: 0.7516 - val_loss: 0.8534 - val_acc: 0.7717\n",
      "Epoch 19/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8375 - acc: 0.7509 - val_loss: 0.8565 - val_acc: 0.7894\n",
      "Epoch 20/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8373 - acc: 0.7599 - val_loss: 0.8492 - val_acc: 0.8085\n",
      "Epoch 21/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8366 - acc: 0.7725 - val_loss: 0.8536 - val_acc: 0.7865\n",
      "Epoch 22/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8363 - acc: 0.7759 - val_loss: 0.8444 - val_acc: 0.7909\n",
      "Epoch 23/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8360 - acc: 0.7828 - val_loss: 0.8462 - val_acc: 0.8056\n",
      "Epoch 24/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8357 - acc: 0.7880 - val_loss: 0.8487 - val_acc: 0.8336\n",
      "Epoch 25/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8354 - acc: 0.7950 - val_loss: 0.8540 - val_acc: 0.8056\n",
      "Epoch 26/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8352 - acc: 0.7998 - val_loss: 0.8511 - val_acc: 0.8130\n",
      "Epoch 27/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8351 - acc: 0.8025 - val_loss: 0.8574 - val_acc: 0.7982\n",
      "Epoch 28/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8348 - acc: 0.8008 - val_loss: 0.8464 - val_acc: 0.8056\n",
      "Epoch 29/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8346 - acc: 0.8073 - val_loss: 0.8451 - val_acc: 0.8306\n",
      "Epoch 30/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8345 - acc: 0.8150 - val_loss: 0.8492 - val_acc: 0.7968\n",
      "Epoch 31/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8344 - acc: 0.8101 - val_loss: 0.8489 - val_acc: 0.7923\n",
      "Epoch 32/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8343 - acc: 0.8155 - val_loss: 0.8510 - val_acc: 0.8041\n",
      "Epoch 33/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8343 - acc: 0.8202 - val_loss: 0.8512 - val_acc: 0.7968\n",
      "Epoch 34/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8341 - acc: 0.8220 - val_loss: 0.8616 - val_acc: 0.7865\n",
      "Epoch 35/100\n",
      "20167/20167 [==============================] - 36s 2ms/step - loss: 0.8340 - acc: 0.8285 - val_loss: 0.8432 - val_acc: 0.8144\n",
      "Epoch 36/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8340 - acc: 0.8277 - val_loss: 0.8406 - val_acc: 0.8174\n",
      "Epoch 37/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8339 - acc: 0.8330 - val_loss: 0.8407 - val_acc: 0.8100\n",
      "Epoch 38/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8340 - acc: 0.8356 - val_loss: 0.8503 - val_acc: 0.8233\n",
      "Epoch 39/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8339 - acc: 0.8401 - val_loss: 0.8403 - val_acc: 0.8027\n",
      "Epoch 40/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8338 - acc: 0.8365 - val_loss: 0.8493 - val_acc: 0.7997\n",
      "Epoch 41/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8338 - acc: 0.8379 - val_loss: 0.8447 - val_acc: 0.8292\n",
      "Epoch 42/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8337 - acc: 0.8403 - val_loss: 0.8412 - val_acc: 0.8262\n",
      "Epoch 43/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8337 - acc: 0.8394 - val_loss: 0.8401 - val_acc: 0.8144\n",
      "Epoch 44/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8336 - acc: 0.8415 - val_loss: 0.8414 - val_acc: 0.7982\n",
      "Epoch 45/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20167/20167 [==============================] - 34s 2ms/step - loss: 0.8336 - acc: 0.8427 - val_loss: 0.8398 - val_acc: 0.8513\n",
      "Epoch 46/100\n",
      "20167/20167 [==============================] - 34s 2ms/step - loss: 0.8336 - acc: 0.8411 - val_loss: 0.8478 - val_acc: 0.7894\n",
      "Epoch 47/100\n",
      "20167/20167 [==============================] - 34s 2ms/step - loss: 0.8335 - acc: 0.8468 - val_loss: 0.8382 - val_acc: 0.8395\n",
      "Epoch 48/100\n",
      "20167/20167 [==============================] - 34s 2ms/step - loss: 0.8336 - acc: 0.8471 - val_loss: 0.8424 - val_acc: 0.7982\n",
      "Epoch 49/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8336 - acc: 0.8453 - val_loss: 0.8467 - val_acc: 0.8159\n",
      "Epoch 50/100\n",
      "20167/20167 [==============================] - 34s 2ms/step - loss: 0.8336 - acc: 0.8415 - val_loss: 0.8426 - val_acc: 0.7982\n",
      "Epoch 51/100\n",
      "20167/20167 [==============================] - 34s 2ms/step - loss: 0.8336 - acc: 0.8468 - val_loss: 0.8387 - val_acc: 0.8306\n",
      "Epoch 52/100\n",
      "20167/20167 [==============================] - 34s 2ms/step - loss: 0.8336 - acc: 0.8505 - val_loss: 0.8393 - val_acc: 0.8218\n",
      "Epoch 53/100\n",
      "20167/20167 [==============================] - 34s 2ms/step - loss: 0.8335 - acc: 0.8418 - val_loss: 0.8440 - val_acc: 0.8306\n",
      "Epoch 54/100\n",
      "20167/20167 [==============================] - 34s 2ms/step - loss: 0.8335 - acc: 0.8429 - val_loss: 0.8504 - val_acc: 0.8071\n",
      "Epoch 55/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8335 - acc: 0.8426 - val_loss: 0.8438 - val_acc: 0.8012\n",
      "Epoch 56/100\n",
      "20167/20167 [==============================] - 34s 2ms/step - loss: 0.8336 - acc: 0.8400 - val_loss: 0.8613 - val_acc: 0.8247\n",
      "Epoch 57/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8336 - acc: 0.8349 - val_loss: 0.8432 - val_acc: 0.8336\n",
      "Epoch 58/100\n",
      "20167/20167 [==============================] - 34s 2ms/step - loss: 0.8335 - acc: 0.8429 - val_loss: 0.8513 - val_acc: 0.8351\n",
      "Epoch 59/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8335 - acc: 0.8396 - val_loss: 0.8387 - val_acc: 0.8321\n",
      "Epoch 60/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8335 - acc: 0.8420 - val_loss: 0.8484 - val_acc: 0.8144\n",
      "Epoch 61/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8335 - acc: 0.8349 - val_loss: 0.8385 - val_acc: 0.8395\n",
      "Epoch 62/100\n",
      "20167/20167 [==============================] - 34s 2ms/step - loss: 0.8335 - acc: 0.8352 - val_loss: 0.8578 - val_acc: 0.8027\n",
      "Epoch 63/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8335 - acc: 0.8381 - val_loss: 0.8422 - val_acc: 0.8277\n",
      "Epoch 64/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8335 - acc: 0.8372 - val_loss: 0.8469 - val_acc: 0.8203\n",
      "Epoch 65/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8335 - acc: 0.8437 - val_loss: 0.8445 - val_acc: 0.7982\n",
      "Epoch 66/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8335 - acc: 0.8410 - val_loss: 0.8482 - val_acc: 0.8056\n",
      "Epoch 67/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8334 - acc: 0.8432 - val_loss: 0.8514 - val_acc: 0.8189\n",
      "Epoch 68/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8334 - acc: 0.8458 - val_loss: 0.8442 - val_acc: 0.8100\n",
      "Epoch 69/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8334 - acc: 0.8431 - val_loss: 0.8458 - val_acc: 0.7982\n",
      "Epoch 70/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8334 - acc: 0.8502 - val_loss: 0.8580 - val_acc: 0.8027\n",
      "Epoch 71/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8334 - acc: 0.8456 - val_loss: 0.8414 - val_acc: 0.8306\n",
      "Epoch 72/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8334 - acc: 0.8448 - val_loss: 0.8421 - val_acc: 0.7688\n",
      "Epoch 73/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8334 - acc: 0.8551 - val_loss: 0.8427 - val_acc: 0.8321\n",
      "Epoch 74/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8334 - acc: 0.8444 - val_loss: 0.8537 - val_acc: 0.7982\n",
      "Epoch 75/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8334 - acc: 0.8520 - val_loss: 0.8417 - val_acc: 0.8012\n",
      "Epoch 76/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8334 - acc: 0.8494 - val_loss: 0.8395 - val_acc: 0.8247\n",
      "Epoch 77/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8334 - acc: 0.8508 - val_loss: 0.8366 - val_acc: 0.8159\n",
      "Epoch 78/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8334 - acc: 0.8442 - val_loss: 0.8400 - val_acc: 0.8100\n",
      "Epoch 79/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8334 - acc: 0.8490 - val_loss: 0.8387 - val_acc: 0.8277\n",
      "Epoch 80/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8334 - acc: 0.8516 - val_loss: 0.8407 - val_acc: 0.8012\n",
      "Epoch 81/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8334 - acc: 0.8476 - val_loss: 0.8356 - val_acc: 0.8409\n",
      "Epoch 82/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8334 - acc: 0.8455 - val_loss: 0.8357 - val_acc: 0.8174\n",
      "Epoch 83/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8334 - acc: 0.8434 - val_loss: 0.8443 - val_acc: 0.8027\n",
      "Epoch 84/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8334 - acc: 0.8486 - val_loss: 0.8370 - val_acc: 0.8174\n",
      "Epoch 85/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8334 - acc: 0.8478 - val_loss: 0.8403 - val_acc: 0.7953\n",
      "Epoch 86/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8334 - acc: 0.8542 - val_loss: 0.8383 - val_acc: 0.8247\n",
      "Epoch 87/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8334 - acc: 0.8533 - val_loss: 0.8422 - val_acc: 0.7865\n",
      "Epoch 88/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8334 - acc: 0.8420 - val_loss: 0.8402 - val_acc: 0.8115\n",
      "Epoch 89/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8334 - acc: 0.8500 - val_loss: 0.8541 - val_acc: 0.7614\n",
      "Epoch 90/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8334 - acc: 0.8506 - val_loss: 0.8382 - val_acc: 0.8498\n",
      "Epoch 91/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8334 - acc: 0.8536 - val_loss: 0.8414 - val_acc: 0.8233\n",
      "Epoch 92/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8334 - acc: 0.8403 - val_loss: 0.8362 - val_acc: 0.8498\n",
      "Epoch 93/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8334 - acc: 0.8555 - val_loss: 0.8393 - val_acc: 0.8174\n",
      "Epoch 94/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8334 - acc: 0.8467 - val_loss: 0.8421 - val_acc: 0.8174\n",
      "Epoch 95/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8334 - acc: 0.8402 - val_loss: 0.8402 - val_acc: 0.8174\n",
      "Epoch 96/100\n",
      "20167/20167 [==============================] - 35s 2ms/step - loss: 0.8334 - acc: 0.8431 - val_loss: 0.8366 - val_acc: 0.8262\n",
      "Epoch 97/100\n",
      "20167/20167 [==============================] - 36s 2ms/step - loss: 0.8334 - acc: 0.8437 - val_loss: 0.8376 - val_acc: 0.8056\n",
      "Epoch 98/100\n",
      "20167/20167 [==============================] - 37s 2ms/step - loss: 0.8334 - acc: 0.8440 - val_loss: 0.8403 - val_acc: 0.8321\n",
      "Epoch 99/100\n",
      "20167/20167 [==============================] - 37s 2ms/step - loss: 0.8334 - acc: 0.8551 - val_loss: 0.8359 - val_acc: 0.8233\n",
      "Epoch 100/100\n",
      "20167/20167 [==============================] - 42s 2ms/step - loss: 0.8334 - acc: 0.8592 - val_loss: 0.8358 - val_acc: 0.8041\n",
      "679/679 [==============================] - 1s 739us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8358032856142047, 0.8041237113402062]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('ConvNet Template 2017 {}'.format(data_input_file))\n",
    "\n",
    "i = 0\n",
    "train_idx = folds[i][0]\n",
    "test_idx = folds[i][1]\n",
    "\n",
    "X_train, y_train = X[train_idx], y[train_idx]\n",
    "X_test, y_test = X[test_idx], y[test_idx]\n",
    "X_train = X_train.squeeze()\n",
    "X_test = X_test.squeeze()\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# Create Model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(\n",
    "    BinaryConv1D(\n",
    "        kernel_size=10,\n",
    "        filters=64,\n",
    "        padding='same',\n",
    "        input_shape=(100,3),\n",
    "        kernel_lr_multiplier=10,use_bias=False))\n",
    "model.add(BatchNormalization(momentum=0.1, epsilon=0.0001))\n",
    "model.add(Activation(binary_tanh_op))\n",
    "\n",
    "\n",
    "model.add(\n",
    "    BinaryConv1D(\n",
    "        kernel_size=10, filters=128, padding='same',\n",
    "        kernel_lr_multiplier=10,use_bias=False))\n",
    "model.add(BatchNormalization(momentum=0.1, epsilon=0.0001))\n",
    "model.add(Activation(binary_tanh_op))\n",
    "\n",
    "\n",
    "model.add(\n",
    "    BinaryConv1D(\n",
    "        kernel_size=10, filters=128, padding='same',\n",
    "        kernel_lr_multiplier=10,use_bias=False))\n",
    "model.add(BatchNormalization(momentum=0.1, epsilon=0.0001))\n",
    "model.add(Activation(binary_tanh_op))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(6, use_bias=False))\n",
    "model.add(BatchNormalization(momentum=0.1, epsilon=0.0001))\n",
    "\n",
    "# Train\n",
    "adam = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "model.compile(loss=squared_hinge, optimizer=adam, metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train, batch_size=128, epochs=100, validation_data=(X_test,y_test), verbose=1 )\n",
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet Template 2017 data/LOSO/WISDM.npz\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 100, 64)           1984      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 100, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 100, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 100, 64)           41024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 100, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 100, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 6400)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 38400     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 6)                 24        \n",
      "=================================================================\n",
      "Total params: 81,944\n",
      "Trainable params: 81,676\n",
      "Non-trainable params: 268\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /home/ankdesh/virtualenvs/bnns/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 20167 samples, validate on 679 samples\n",
      "Epoch 1/100\n",
      "20167/20167 [==============================] - 9s 446us/step - loss: 0.8718 - acc: 0.5183 - val_loss: 0.8389 - val_acc: 0.7570\n",
      "Epoch 2/100\n",
      "20167/20167 [==============================] - 8s 401us/step - loss: 0.8448 - acc: 0.6535 - val_loss: 0.8440 - val_acc: 0.7761\n",
      "Epoch 3/100\n",
      "20167/20167 [==============================] - 8s 407us/step - loss: 0.8388 - acc: 0.7031 - val_loss: 0.8401 - val_acc: 0.7747\n",
      "Epoch 4/100\n",
      "20167/20167 [==============================] - 8s 404us/step - loss: 0.8365 - acc: 0.7320 - val_loss: 0.8360 - val_acc: 0.8027\n",
      "Epoch 5/100\n",
      "20167/20167 [==============================] - 8s 405us/step - loss: 0.8353 - acc: 0.7568 - val_loss: 0.8375 - val_acc: 0.8100\n",
      "Epoch 6/100\n",
      "20167/20167 [==============================] - 8s 404us/step - loss: 0.8346 - acc: 0.7687 - val_loss: 0.8508 - val_acc: 0.7820\n",
      "Epoch 7/100\n",
      "20167/20167 [==============================] - 8s 403us/step - loss: 0.8342 - acc: 0.7756 - val_loss: 0.8366 - val_acc: 0.8115\n",
      "Epoch 8/100\n",
      "20167/20167 [==============================] - 8s 406us/step - loss: 0.8340 - acc: 0.7825 - val_loss: 0.8369 - val_acc: 0.8748\n",
      "Epoch 9/100\n",
      "20167/20167 [==============================] - 8s 404us/step - loss: 0.8338 - acc: 0.7945 - val_loss: 0.8367 - val_acc: 0.7968\n",
      "Epoch 10/100\n",
      "20167/20167 [==============================] - 8s 405us/step - loss: 0.8338 - acc: 0.7977 - val_loss: 0.8354 - val_acc: 0.8733\n",
      "Epoch 11/100\n",
      "20167/20167 [==============================] - 8s 407us/step - loss: 0.8336 - acc: 0.8049 - val_loss: 0.8392 - val_acc: 0.8483\n",
      "Epoch 12/100\n",
      "20167/20167 [==============================] - 8s 403us/step - loss: 0.8336 - acc: 0.8183 - val_loss: 0.8350 - val_acc: 0.8940\n",
      "Epoch 13/100\n",
      "20167/20167 [==============================] - 8s 404us/step - loss: 0.8335 - acc: 0.8192 - val_loss: 0.8368 - val_acc: 0.8675\n",
      "Epoch 14/100\n",
      "20167/20167 [==============================] - 8s 406us/step - loss: 0.8335 - acc: 0.8233 - val_loss: 0.8346 - val_acc: 0.8689\n",
      "Epoch 15/100\n",
      "20167/20167 [==============================] - 8s 405us/step - loss: 0.8334 - acc: 0.8315 - val_loss: 0.8352 - val_acc: 0.8424\n",
      "Epoch 16/100\n",
      "20167/20167 [==============================] - 8s 404us/step - loss: 0.8335 - acc: 0.8307 - val_loss: 0.8356 - val_acc: 0.9102\n",
      "Epoch 17/100\n",
      "20167/20167 [==============================] - 8s 405us/step - loss: 0.8334 - acc: 0.8395 - val_loss: 0.8349 - val_acc: 0.8792\n",
      "Epoch 18/100\n",
      "20167/20167 [==============================] - 8s 404us/step - loss: 0.8334 - acc: 0.8381 - val_loss: 0.8343 - val_acc: 0.8262\n",
      "Epoch 19/100\n",
      "20167/20167 [==============================] - 8s 407us/step - loss: 0.8335 - acc: 0.8284 - val_loss: 0.8348 - val_acc: 0.8306\n",
      "Epoch 20/100\n",
      "20167/20167 [==============================] - 8s 405us/step - loss: 0.8334 - acc: 0.8352 - val_loss: 0.8353 - val_acc: 0.8395\n",
      "Epoch 21/100\n",
      "20167/20167 [==============================] - 8s 405us/step - loss: 0.8334 - acc: 0.8432 - val_loss: 0.8361 - val_acc: 0.7982\n",
      "Epoch 22/100\n",
      "20167/20167 [==============================] - 8s 407us/step - loss: 0.8334 - acc: 0.8312 - val_loss: 0.8380 - val_acc: 0.8306\n",
      "Epoch 23/100\n",
      "20167/20167 [==============================] - 8s 407us/step - loss: 0.8334 - acc: 0.8348 - val_loss: 0.8358 - val_acc: 0.7806\n",
      "Epoch 24/100\n",
      "20167/20167 [==============================] - 8s 405us/step - loss: 0.8334 - acc: 0.8651 - val_loss: 0.8364 - val_acc: 0.8351\n",
      "Epoch 25/100\n",
      "20167/20167 [==============================] - 8s 406us/step - loss: 0.8334 - acc: 0.8595 - val_loss: 0.8413 - val_acc: 0.8395\n",
      "Epoch 26/100\n",
      "20167/20167 [==============================] - 8s 408us/step - loss: 0.8334 - acc: 0.8534 - val_loss: 0.8346 - val_acc: 0.9426\n",
      "Epoch 27/100\n",
      "20167/20167 [==============================] - 8s 408us/step - loss: 0.8334 - acc: 0.8559 - val_loss: 0.8367 - val_acc: 0.8380\n",
      "Epoch 28/100\n",
      "20167/20167 [==============================] - 8s 410us/step - loss: 0.8334 - acc: 0.8538 - val_loss: 0.8335 - val_acc: 0.8306\n",
      "Epoch 29/100\n",
      "20167/20167 [==============================] - 8s 406us/step - loss: 0.8334 - acc: 0.8649 - val_loss: 0.8340 - val_acc: 0.8292\n",
      "Epoch 30/100\n",
      "20167/20167 [==============================] - 8s 409us/step - loss: 0.8334 - acc: 0.8589 - val_loss: 0.8338 - val_acc: 0.9367\n",
      "Epoch 31/100\n",
      "20167/20167 [==============================] - 8s 406us/step - loss: 0.8333 - acc: 0.8677 - val_loss: 0.8349 - val_acc: 0.8925\n",
      "Epoch 32/100\n",
      "20167/20167 [==============================] - 8s 406us/step - loss: 0.8334 - acc: 0.8608 - val_loss: 0.8336 - val_acc: 0.9470\n",
      "Epoch 33/100\n",
      "20167/20167 [==============================] - 8s 413us/step - loss: 0.8334 - acc: 0.8522 - val_loss: 0.8336 - val_acc: 0.8277\n",
      "Epoch 34/100\n",
      "20167/20167 [==============================] - 8s 408us/step - loss: 0.8333 - acc: 0.8580 - val_loss: 0.8346 - val_acc: 0.8807\n",
      "Epoch 35/100\n",
      "20167/20167 [==============================] - 8s 406us/step - loss: 0.8333 - acc: 0.8543 - val_loss: 0.8339 - val_acc: 0.8733\n",
      "Epoch 36/100\n",
      "20167/20167 [==============================] - 8s 407us/step - loss: 0.8334 - acc: 0.8569 - val_loss: 0.8341 - val_acc: 0.8940\n",
      "Epoch 37/100\n",
      "20167/20167 [==============================] - 8s 410us/step - loss: 0.8333 - acc: 0.8542 - val_loss: 0.8345 - val_acc: 0.9087\n",
      "Epoch 38/100\n",
      "20167/20167 [==============================] - 8s 410us/step - loss: 0.8334 - acc: 0.8465 - val_loss: 0.8343 - val_acc: 0.7658\n",
      "Epoch 39/100\n",
      "20167/20167 [==============================] - 8s 407us/step - loss: 0.8333 - acc: 0.8472 - val_loss: 0.8341 - val_acc: 0.8380\n",
      "Epoch 40/100\n",
      "20167/20167 [==============================] - 8s 407us/step - loss: 0.8333 - acc: 0.8468 - val_loss: 0.8339 - val_acc: 0.7909\n",
      "Epoch 41/100\n",
      "20167/20167 [==============================] - 8s 409us/step - loss: 0.8334 - acc: 0.8423 - val_loss: 0.8336 - val_acc: 0.8866\n",
      "Epoch 42/100\n",
      "20167/20167 [==============================] - 8s 409us/step - loss: 0.8334 - acc: 0.8405 - val_loss: 0.8337 - val_acc: 0.8395\n",
      "Epoch 43/100\n",
      "20167/20167 [==============================] - 8s 412us/step - loss: 0.8333 - acc: 0.8420 - val_loss: 0.8337 - val_acc: 0.8174\n",
      "Epoch 44/100\n",
      "20167/20167 [==============================] - 8s 410us/step - loss: 0.8333 - acc: 0.8445 - val_loss: 0.8338 - val_acc: 0.8719\n",
      "Epoch 45/100\n",
      "20167/20167 [==============================] - 8s 416us/step - loss: 0.8333 - acc: 0.8425 - val_loss: 0.8338 - val_acc: 0.9057\n",
      "Epoch 46/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20167/20167 [==============================] - 8s 383us/step - loss: 0.8333 - acc: 0.8350 - val_loss: 0.8345 - val_acc: 0.8601\n",
      "Epoch 47/100\n",
      "20167/20167 [==============================] - 8s 382us/step - loss: 0.8333 - acc: 0.8415 - val_loss: 0.8339 - val_acc: 0.8925\n",
      "Epoch 48/100\n",
      "20167/20167 [==============================] - 8s 383us/step - loss: 0.8333 - acc: 0.8458 - val_loss: 0.8336 - val_acc: 0.8733\n",
      "Epoch 49/100\n",
      "20167/20167 [==============================] - 8s 383us/step - loss: 0.8333 - acc: 0.8463 - val_loss: 0.8369 - val_acc: 0.7054\n",
      "Epoch 50/100\n",
      "20167/20167 [==============================] - 8s 384us/step - loss: 0.8333 - acc: 0.8570 - val_loss: 0.8348 - val_acc: 0.8542\n",
      "Epoch 51/100\n",
      "20167/20167 [==============================] - 8s 382us/step - loss: 0.8333 - acc: 0.8593 - val_loss: 0.8340 - val_acc: 0.7865\n",
      "Epoch 52/100\n",
      "20167/20167 [==============================] - 8s 385us/step - loss: 0.8333 - acc: 0.8519 - val_loss: 0.8335 - val_acc: 0.7997\n",
      "Epoch 53/100\n",
      "20167/20167 [==============================] - 8s 383us/step - loss: 0.8333 - acc: 0.8507 - val_loss: 0.8337 - val_acc: 0.8380\n",
      "Epoch 54/100\n",
      "20167/20167 [==============================] - 8s 386us/step - loss: 0.8333 - acc: 0.8423 - val_loss: 0.8351 - val_acc: 0.8085\n",
      "Epoch 55/100\n",
      "20167/20167 [==============================] - 8s 383us/step - loss: 0.8333 - acc: 0.8351 - val_loss: 0.8337 - val_acc: 0.9028\n",
      "Epoch 56/100\n",
      "20167/20167 [==============================] - 8s 383us/step - loss: 0.8333 - acc: 0.8405 - val_loss: 0.8345 - val_acc: 0.9278\n",
      "Epoch 57/100\n",
      "20167/20167 [==============================] - 8s 386us/step - loss: 0.8334 - acc: 0.8229 - val_loss: 0.8349 - val_acc: 0.8645\n",
      "Epoch 58/100\n",
      "20167/20167 [==============================] - 8s 386us/step - loss: 0.8333 - acc: 0.8367 - val_loss: 0.8342 - val_acc: 0.8012\n",
      "Epoch 59/100\n",
      "20167/20167 [==============================] - 8s 384us/step - loss: 0.8333 - acc: 0.8371 - val_loss: 0.8335 - val_acc: 0.8483\n",
      "Epoch 60/100\n",
      "20167/20167 [==============================] - 8s 386us/step - loss: 0.8333 - acc: 0.8422 - val_loss: 0.8338 - val_acc: 0.8203\n",
      "Epoch 61/100\n",
      "20167/20167 [==============================] - 8s 384us/step - loss: 0.8333 - acc: 0.8378 - val_loss: 0.8340 - val_acc: 0.8586\n",
      "Epoch 62/100\n",
      "20167/20167 [==============================] - 8s 386us/step - loss: 0.8333 - acc: 0.8525 - val_loss: 0.8347 - val_acc: 0.8616\n",
      "Epoch 63/100\n",
      "20167/20167 [==============================] - 8s 386us/step - loss: 0.8334 - acc: 0.8352 - val_loss: 0.8338 - val_acc: 0.8630\n",
      "Epoch 64/100\n",
      "20167/20167 [==============================] - 8s 388us/step - loss: 0.8333 - acc: 0.8245 - val_loss: 0.8348 - val_acc: 0.7938\n",
      "Epoch 65/100\n",
      "20167/20167 [==============================] - 8s 386us/step - loss: 0.8333 - acc: 0.8346 - val_loss: 0.8338 - val_acc: 0.9116\n",
      "Epoch 66/100\n",
      "20167/20167 [==============================] - 8s 384us/step - loss: 0.8333 - acc: 0.8364 - val_loss: 0.8336 - val_acc: 0.8306\n",
      "Epoch 67/100\n",
      "20167/20167 [==============================] - 8s 385us/step - loss: 0.8333 - acc: 0.8335 - val_loss: 0.8335 - val_acc: 0.7850\n",
      "Epoch 68/100\n",
      "20167/20167 [==============================] - 8s 386us/step - loss: 0.8333 - acc: 0.8373 - val_loss: 0.8343 - val_acc: 0.8262\n",
      "Epoch 69/100\n",
      "20167/20167 [==============================] - 8s 386us/step - loss: 0.8333 - acc: 0.8460 - val_loss: 0.8338 - val_acc: 0.8130\n",
      "Epoch 70/100\n",
      "20167/20167 [==============================] - 8s 386us/step - loss: 0.8333 - acc: 0.8551 - val_loss: 0.8335 - val_acc: 0.9116\n",
      "Epoch 71/100\n",
      "20167/20167 [==============================] - 8s 387us/step - loss: 0.8333 - acc: 0.8630 - val_loss: 0.8334 - val_acc: 0.8719\n",
      "Epoch 72/100\n",
      "20167/20167 [==============================] - 8s 389us/step - loss: 0.8334 - acc: 0.8405 - val_loss: 0.8338 - val_acc: 0.9057\n",
      "Epoch 73/100\n",
      "20167/20167 [==============================] - 8s 387us/step - loss: 0.8333 - acc: 0.8547 - val_loss: 0.8336 - val_acc: 0.8719\n",
      "Epoch 74/100\n",
      "20167/20167 [==============================] - 8s 389us/step - loss: 0.8333 - acc: 0.8642 - val_loss: 0.8337 - val_acc: 0.9264\n",
      "Epoch 75/100\n",
      "20167/20167 [==============================] - 8s 388us/step - loss: 0.8333 - acc: 0.8632 - val_loss: 0.8346 - val_acc: 0.7909\n",
      "Epoch 76/100\n",
      "20167/20167 [==============================] - 8s 388us/step - loss: 0.8333 - acc: 0.8527 - val_loss: 0.8338 - val_acc: 0.8542\n",
      "Epoch 77/100\n",
      "20167/20167 [==============================] - 8s 388us/step - loss: 0.8333 - acc: 0.8671 - val_loss: 0.8340 - val_acc: 0.8262\n",
      "Epoch 78/100\n",
      "20167/20167 [==============================] - 8s 386us/step - loss: 0.8333 - acc: 0.8624 - val_loss: 0.8335 - val_acc: 0.8159\n",
      "Epoch 79/100\n",
      "20167/20167 [==============================] - 8s 391us/step - loss: 0.8333 - acc: 0.8517 - val_loss: 0.8334 - val_acc: 0.8056\n",
      "Epoch 80/100\n",
      "20167/20167 [==============================] - 8s 388us/step - loss: 0.8333 - acc: 0.8478 - val_loss: 0.8366 - val_acc: 0.7305\n",
      "Epoch 81/100\n",
      "20167/20167 [==============================] - 8s 387us/step - loss: 0.8333 - acc: 0.8513 - val_loss: 0.8342 - val_acc: 0.7099\n",
      "Epoch 82/100\n",
      "20167/20167 [==============================] - 8s 387us/step - loss: 0.8333 - acc: 0.8442 - val_loss: 0.8338 - val_acc: 0.8424\n",
      "Epoch 83/100\n",
      "20167/20167 [==============================] - 8s 388us/step - loss: 0.8333 - acc: 0.8363 - val_loss: 0.8342 - val_acc: 0.8409\n",
      "Epoch 84/100\n",
      "20167/20167 [==============================] - 8s 387us/step - loss: 0.8333 - acc: 0.8378 - val_loss: 0.8352 - val_acc: 0.7408\n",
      "Epoch 85/100\n",
      "20167/20167 [==============================] - 8s 391us/step - loss: 0.8333 - acc: 0.8480 - val_loss: 0.8336 - val_acc: 0.7909\n",
      "Epoch 86/100\n",
      "20167/20167 [==============================] - 8s 387us/step - loss: 0.8333 - acc: 0.8329 - val_loss: 0.8339 - val_acc: 0.8704\n",
      "Epoch 87/100\n",
      "20167/20167 [==============================] - 8s 390us/step - loss: 0.8333 - acc: 0.8451 - val_loss: 0.8348 - val_acc: 0.8218\n",
      "Epoch 88/100\n",
      "20167/20167 [==============================] - 8s 417us/step - loss: 0.8333 - acc: 0.8326 - val_loss: 0.8350 - val_acc: 0.7320\n",
      "Epoch 89/100\n",
      "20167/20167 [==============================] - 8s 418us/step - loss: 0.8333 - acc: 0.8329 - val_loss: 0.8336 - val_acc: 0.8454\n",
      "Epoch 90/100\n",
      "20167/20167 [==============================] - 7s 365us/step - loss: 0.8333 - acc: 0.8372 - val_loss: 0.8335 - val_acc: 0.7865\n",
      "Epoch 91/100\n",
      "20167/20167 [==============================] - 7s 357us/step - loss: 0.8333 - acc: 0.8407 - val_loss: 0.8338 - val_acc: 0.7865\n",
      "Epoch 92/100\n",
      "20167/20167 [==============================] - 7s 354us/step - loss: 0.8333 - acc: 0.8501 - val_loss: 0.8335 - val_acc: 0.9013\n",
      "Epoch 93/100\n",
      "20167/20167 [==============================] - 7s 355us/step - loss: 0.8333 - acc: 0.8534 - val_loss: 0.8342 - val_acc: 0.8321\n",
      "Epoch 94/100\n",
      "20167/20167 [==============================] - 7s 358us/step - loss: 0.8333 - acc: 0.8467 - val_loss: 0.8340 - val_acc: 0.7570\n",
      "Epoch 95/100\n",
      "20167/20167 [==============================] - 7s 354us/step - loss: 0.8333 - acc: 0.8266 - val_loss: 0.8338 - val_acc: 0.8616\n",
      "Epoch 96/100\n",
      "20167/20167 [==============================] - 7s 353us/step - loss: 0.8333 - acc: 0.8282 - val_loss: 0.8336 - val_acc: 0.7909\n",
      "Epoch 97/100\n",
      "20167/20167 [==============================] - 7s 355us/step - loss: 0.8333 - acc: 0.8137 - val_loss: 0.8335 - val_acc: 0.8027\n",
      "Epoch 98/100\n",
      "20167/20167 [==============================] - 7s 354us/step - loss: 0.8333 - acc: 0.8243 - val_loss: 0.8338 - val_acc: 0.8395\n",
      "Epoch 99/100\n",
      "20167/20167 [==============================] - 7s 355us/step - loss: 0.8333 - acc: 0.8297 - val_loss: 0.8385 - val_acc: 0.4742\n",
      "Epoch 100/100\n",
      "20167/20167 [==============================] - 7s 356us/step - loss: 0.8333 - acc: 0.8122 - val_loss: 0.8336 - val_acc: 0.8365\n",
      "679/679 [==============================] - 0s 125us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8336257227039477, 0.8365243004418262]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_input_file = 'data/LOSO/WISDM.npz'\n",
    "\n",
    "tmp = np.load(data_input_file)\n",
    "X = tmp['X']\n",
    "y = tmp['y']\n",
    "folds = tmp['folds']\n",
    "\n",
    "n_class = y.shape[1]\n",
    "\n",
    "avg_acc = []\n",
    "avg_recall = []\n",
    "avg_f1 = []\n",
    "y = np.argmax(y, axis=1)\n",
    "\n",
    "print('ConvNet Template 2017 {}'.format(data_input_file))\n",
    "\n",
    "i = 0\n",
    "train_idx = folds[i][0]\n",
    "test_idx = folds[i][1]\n",
    "\n",
    "X_train, y_train = X[train_idx], y[train_idx]\n",
    "X_test, y_test = X[test_idx], y[test_idx]\n",
    "X_train = X_train.squeeze()\n",
    "X_test = X_test.squeeze()\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# Create Model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(\n",
    "    Conv1D(\n",
    "        kernel_size=10,\n",
    "        filters=64,\n",
    "        padding='same',\n",
    "        input_shape=(100,3)))\n",
    "model.add(BatchNormalization(momentum=0.1, epsilon=0.0001))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(\n",
    "        kernel_size=10, filters=64, padding='same'))\n",
    "model.add(BatchNormalization(momentum=0.1, epsilon=0.0001))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(6, use_bias=False))\n",
    "model.add(BatchNormalization(momentum=0.1, epsilon=0.0001))\n",
    "\n",
    "# Train\n",
    "adam = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "model.compile(loss=squared_hinge, optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "model.fit(\n",
    "    X_train, y_train, batch_size=50, epochs=100, validation_data=(X_test,y_test))\n",
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248/248 [==============================] - 0s 191us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9168156596922106, 0.6653225806451613]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

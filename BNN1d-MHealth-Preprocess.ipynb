{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.classification import accuracy_score, recall_score, f1_score\n",
    "import scipy.stats as st\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "sys.path.append('../quantnn-keras/layers/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.losses import squared_hinge\n",
    "from keras.layers import Flatten, Dense, Activation, BatchNormalization, Conv1D\n",
    "from keras.callbacks import ReduceLROnPlateau, TensorBoard\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "import numpy as np\n",
    "\n",
    "from binary_layers import BinaryConv2D, BinaryConv1D\n",
    "from binary_ops import binary_tanh as binary_tanh_op\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def A(sample):\n",
    "    feat = []\n",
    "    for col in range(0,sample.shape[1]):\n",
    "        average = np.average(sample[:,col])\n",
    "        feat.append(average)\n",
    "\n",
    "    return feat\n",
    "\n",
    "def SD(sample):\n",
    "    feat = []\n",
    "    for col in range(0, sample.shape[1]):\n",
    "        std = np.std(sample[:, col])\n",
    "        feat.append(std)\n",
    "\n",
    "    return feat\n",
    "\n",
    "def AAD(sample):\n",
    "    feat = []\n",
    "    for col in range(0, sample.shape[1]):\n",
    "        data = sample[:, col]\n",
    "        add = np.mean(np.absolute(data - np.mean(data)))\n",
    "        feat.append(add)\n",
    "\n",
    "    return feat\n",
    "\n",
    "def ARA(sample):\n",
    "    #Average Resultant Acceleration[1]:\n",
    "    # Average of the square roots of the sum of the values of each axis squared âˆš(xi^2 + yi^2+ zi^2) over the ED\n",
    "    feat = []\n",
    "    sum_square = 0\n",
    "    sample = np.power(sample, 2)\n",
    "    for col in range(0, sample.shape[1]):\n",
    "        sum_square = sum_square + sample[:, col]\n",
    "\n",
    "    sample = np.sqrt(sum_square)\n",
    "    average = np.average(sample)\n",
    "    feat.append(average)\n",
    "    return feat\n",
    "\n",
    "def TBP(sample):\n",
    "    from scipy import signal\n",
    "    feat = []\n",
    "    sum_of_time = 0\n",
    "    for col in range(0, sample.shape[1]):\n",
    "        data = sample[:, col]\n",
    "        peaks = signal.find_peaks_cwt(data, np.arange(1,4))\n",
    "\n",
    "        feat.append(peaks)\n",
    "\n",
    "    return feat\n",
    "\n",
    "def feature_extraction(X):\n",
    "    #Extracts the features, as mentioned by Catal et al. 2015\n",
    "    # Average - A,\n",
    "    # Standard Deviation - SD,\n",
    "    # Average Absolute Difference - AAD,\n",
    "    # Average Resultant Acceleration - ARA(1),\n",
    "    # Time Between Peaks - TBP\n",
    "    X_tmp = []\n",
    "    for sample in X:\n",
    "        features = A(sample)\n",
    "        features = np.hstack((features, A(sample)))\n",
    "        features = np.hstack((features, SD(sample)))\n",
    "        features = np.hstack((features, AAD(sample)))\n",
    "        features = np.hstack((features, ARA(sample)))\n",
    "        #features = np.hstack((features, TBP(sample)))\n",
    "        X_tmp.append(features)\n",
    "\n",
    "    X = np.array(X_tmp)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [ReduceLROnPlateau('val_loss', patience=5, verbose=1), TensorBoard()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input_file = 'data/LOSO/MHEALTH.npz'\n",
    "\n",
    "tmp = np.load(data_input_file)\n",
    "X = tmp['X']\n",
    "y = tmp['y']\n",
    "folds = tmp['folds']\n",
    "\n",
    "n_class = y.shape[1]\n",
    "\n",
    "avg_acc = []\n",
    "avg_recall = []\n",
    "avg_f1 = []\n",
    "y = np.argmax(y, axis=1)\n",
    "\n",
    "print('ConvNet Template 2017 {}'.format(data_input_file))\n",
    "\n",
    "for i in range(0, len(folds)):\n",
    "    train_idx = folds[i][0]\n",
    "    test_idx = folds[i][1]\n",
    "\n",
    "    X_train, y_train = X[train_idx], y[train_idx]\n",
    "    X_test, y_test = X[test_idx], y[test_idx]\n",
    "    \n",
    "    X_train = feature_extraction(X_train)\n",
    "    X_test = feature_extraction(X_test)\n",
    "    \n",
    "    \n",
    "    X_train = X_train.reshape(-1,1001,1)\n",
    "    X_test = X_test.reshape(-1,1001,1)\n",
    "    y_train = to_categorical(y_train)\n",
    "    y_test = to_categorical(y_test)\n",
    "    \n",
    "    # Create Model\n",
    "    model = Sequential()\n",
    "\n",
    "    _momentum = 0.1\n",
    "    _epsilon = 0.001\n",
    "    \n",
    "    model.add(\n",
    "        BinaryConv1D(\n",
    "            kernel_size=10,\n",
    "            filters=64,\n",
    "            padding='same',\n",
    "            input_shape=(1001,1),\n",
    "            kernel_lr_multiplier=10,use_bias=True))\n",
    "    model.add(BatchNormalization(momentum=_momentum, epsilon=_epsilon))\n",
    "    model.add(Activation(binary_tanh_op))\n",
    "\n",
    "\n",
    "    model.add(\n",
    "        BinaryConv1D(\n",
    "            kernel_size=10, filters=128, padding='same',\n",
    "            kernel_lr_multiplier=10,use_bias=True))\n",
    "    model.add(BatchNormalization(momentum=_momentum, epsilon=_epsilon))\n",
    "    model.add(Activation(binary_tanh_op))\n",
    "\n",
    "\n",
    "    model.add(\n",
    "        BinaryConv1D(\n",
    "            kernel_size=10, filters=128, padding='same',\n",
    "            kernel_lr_multiplier=10,use_bias=True))\n",
    "    model.add(BatchNormalization(momentum=_momentum, epsilon=_epsilon))\n",
    "    model.add(Activation(binary_tanh_op))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(12, use_bias=True))\n",
    "    model.add(BatchNormalization(momentum=_momentum, epsilon=_epsilon))\n",
    "\n",
    "    # Train\n",
    "    adam = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    #sgd = SGD(lr=0.001)\n",
    "    model.compile(loss=squared_hinge, optimizer=adam, metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "\n",
    "    model.fit(\n",
    "        X_train, y_train, batch_size=50, epochs=100, validation_data=(X_test,y_test), verbose=0 )\n",
    "    print (model.evaluate(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (X_train.shape, X_test.shape)\n",
    "print (y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(-1,1)\n",
    "X_test = X_test.reshape(-1,1)\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (X_train.shape, X_test.shape)\n",
    "print (y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (folds.shape, len(folds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input_file = 'data/LOSO/MHEALTH.npz'\n",
    "\n",
    "tmp = np.load(data_input_file)\n",
    "X = tmp['X']\n",
    "y = tmp['y']\n",
    "folds = tmp['folds']\n",
    "\n",
    "n_class = y.shape[1]\n",
    "\n",
    "avg_acc = []\n",
    "avg_recall = []\n",
    "avg_f1 = []\n",
    "y = np.argmax(y, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-90c748ab6724>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_extraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_extraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-c82ce406212b>\u001b[0m in \u001b[0;36mfeature_extraction\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAAD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mARA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m#features = np.hstack((features, TBP(sample)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-c82ce406212b>\u001b[0m in \u001b[0;36mAAD\u001b[0;34m(sample)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0madd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabsolute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mfeat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/bnns/lib/python3.5/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   3116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3117\u001b[0m     return _methods._mean(a, axis=axis, dtype=dtype,\n\u001b[0;32m-> 3118\u001b[0;31m                           out=out, **kwargs)\n\u001b[0m\u001b[1;32m   3119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/bnns/lib/python3.5/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mis_float16_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         ret = um.true_divide(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "\n",
    "train_idx = folds[i][0]\n",
    "test_idx = folds[i][1]\n",
    "\n",
    "X_train, y_train = X[train_idx], y[train_idx]\n",
    "X_test, y_test = X[test_idx], y[test_idx]\n",
    "\n",
    "X_train = feature_extraction(X_train)\n",
    "X_test = feature_extraction(X_test)\n",
    "\n",
    "\n",
    "\n",
    "X_train = X_train.reshape(-1,1001,1)\n",
    "X_test = X_test.reshape(-1,1001,1)\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# Create Model\n",
    "model = Sequential()\n",
    "\n",
    "_momentum = 0.1\n",
    "_epsilon = 0.0001\n",
    "\n",
    "model.add(\n",
    "    BinaryConv1D(\n",
    "        kernel_size=10,\n",
    "        filters=64,\n",
    "        padding='same',\n",
    "        input_shape=(1001,1),\n",
    "        kernel_lr_multiplier=10,use_bias=True))\n",
    "model.add(BatchNormalization(momentum=_momentum, epsilon=_epsilon))\n",
    "model.add(Activation(binary_tanh_op))\n",
    "\n",
    "\n",
    "model.add(\n",
    "    BinaryConv1D(\n",
    "        kernel_size=10, filters=128, padding='same',\n",
    "        kernel_lr_multiplier=10,use_bias=True))\n",
    "model.add(BatchNormalization(momentum=_momentum, epsilon=_epsilon))\n",
    "model.add(Activation(binary_tanh_op))\n",
    "\n",
    "\n",
    "model.add(\n",
    "    BinaryConv1D(\n",
    "        kernel_size=10, filters=128, padding='same',\n",
    "        kernel_lr_multiplier=10,use_bias=True))\n",
    "model.add(BatchNormalization(momentum=_momentum, epsilon=_epsilon))\n",
    "model.add(Activation(binary_tanh_op))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(12, use_bias=True))\n",
    "model.add(BatchNormalization(momentum=_momentum, epsilon=_epsilon))\n",
    "\n",
    "# Train\n",
    "adam = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "#sgd = SGD(lr=0.001)\n",
    "model.compile(loss=squared_hinge, optimizer=adam, metrics=['accuracy'])\n",
    "#model.summary()\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train, batch_size=128, epochs=20, validation_data=(X_test,y_test), verbose=1, )\n",
    "print (model.evaluate(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ankdesh/virtualenvs/bnns/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 1001, 64)          64064     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 1001, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1001, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1001, 128)         8192128   \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 1001, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1001, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 1001, 128)         16384128  \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1001, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 1001, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 128128)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 12)                1537536   \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 12)                48        \n",
      "=================================================================\n",
      "Total params: 26,179,184\n",
      "Trainable params: 26,178,520\n",
      "Non-trainable params: 664\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /home/ankdesh/virtualenvs/bnns/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 2300 samples, validate on 255 samples\n",
      "Epoch 1/100\n",
      "2300/2300 [==============================] - 109s 47ms/step - loss: 0.9263 - acc: 0.3535 - val_loss: 0.9167 - val_acc: 0.6706\n",
      "Epoch 2/100\n",
      "2300/2300 [==============================] - 105s 46ms/step - loss: 0.9176 - acc: 0.4152 - val_loss: 0.9167 - val_acc: 0.4784\n",
      "Epoch 3/100\n",
      "2300/2300 [==============================] - 105s 45ms/step - loss: 0.9173 - acc: 0.4187 - val_loss: 0.9167 - val_acc: 0.5529\n",
      "Epoch 4/100\n",
      "2300/2300 [==============================] - 105s 46ms/step - loss: 0.9172 - acc: 0.4452 - val_loss: 0.9167 - val_acc: 0.5843\n",
      "Epoch 5/100\n",
      "2300/2300 [==============================] - 105s 46ms/step - loss: 0.9171 - acc: 0.4304 - val_loss: 0.9167 - val_acc: 0.4275\n",
      "Epoch 6/100\n",
      "2300/2300 [==============================] - 105s 46ms/step - loss: 0.9169 - acc: 0.4683 - val_loss: 0.9167 - val_acc: 0.5882\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "Epoch 7/100\n",
      "2300/2300 [==============================] - 105s 45ms/step - loss: 0.9170 - acc: 0.4665 - val_loss: 0.9167 - val_acc: 0.5490\n",
      "Epoch 8/100\n",
      "2300/2300 [==============================] - 105s 46ms/step - loss: 0.9168 - acc: 0.4839 - val_loss: 0.9167 - val_acc: 0.5686\n",
      "Epoch 9/100\n",
      "2300/2300 [==============================] - 105s 46ms/step - loss: 0.9169 - acc: 0.4870 - val_loss: 0.9167 - val_acc: 0.6588\n",
      "Epoch 10/100\n",
      "2300/2300 [==============================] - 105s 46ms/step - loss: 0.9169 - acc: 0.4778 - val_loss: 0.9167 - val_acc: 0.7216\n",
      "Epoch 11/100\n",
      "2300/2300 [==============================] - 105s 46ms/step - loss: 0.9168 - acc: 0.4813 - val_loss: 0.9167 - val_acc: 0.6118\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "Epoch 12/100\n",
      "2300/2300 [==============================] - 105s 46ms/step - loss: 0.9169 - acc: 0.4626 - val_loss: 0.9167 - val_acc: 0.4745\n",
      "Epoch 13/100\n",
      "2300/2300 [==============================] - 105s 46ms/step - loss: 0.9171 - acc: 0.4752 - val_loss: 0.9167 - val_acc: 0.5765\n",
      "Epoch 14/100\n",
      "2300/2300 [==============================] - 105s 46ms/step - loss: 0.9169 - acc: 0.4457 - val_loss: 0.9167 - val_acc: 0.4510\n",
      "Epoch 15/100\n",
      "2300/2300 [==============================] - 105s 46ms/step - loss: 0.9169 - acc: 0.4704 - val_loss: 0.9168 - val_acc: 0.3255\n",
      "Epoch 16/100\n",
      "2300/2300 [==============================] - 105s 46ms/step - loss: 0.9169 - acc: 0.4743 - val_loss: 0.9167 - val_acc: 0.5333\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
      "Epoch 17/100\n",
      "2300/2300 [==============================] - 105s 45ms/step - loss: 0.9169 - acc: 0.4683 - val_loss: 0.9167 - val_acc: 0.5804\n",
      "Epoch 18/100\n",
      "2300/2300 [==============================] - 105s 45ms/step - loss: 0.9168 - acc: 0.4704 - val_loss: 0.9167 - val_acc: 0.6235\n",
      "Epoch 19/100\n",
      "2300/2300 [==============================] - 105s 46ms/step - loss: 0.9168 - acc: 0.4596 - val_loss: 0.9167 - val_acc: 0.5216\n",
      "Epoch 20/100\n",
      "2300/2300 [==============================] - 104s 45ms/step - loss: 0.9168 - acc: 0.4796 - val_loss: 0.9167 - val_acc: 0.4902\n",
      "Epoch 21/100\n",
      "2300/2300 [==============================] - 105s 46ms/step - loss: 0.9168 - acc: 0.4648 - val_loss: 0.9167 - val_acc: 0.6275\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1.0000000116860975e-08.\n",
      "Epoch 22/100\n",
      "2300/2300 [==============================] - 105s 46ms/step - loss: 0.9169 - acc: 0.4539 - val_loss: 0.9167 - val_acc: 0.5804\n",
      "Epoch 23/100\n",
      "2300/2300 [==============================] - 105s 46ms/step - loss: 0.9169 - acc: 0.4504 - val_loss: 0.9167 - val_acc: 0.5843\n",
      "Epoch 24/100\n",
      "2300/2300 [==============================] - 105s 46ms/step - loss: 0.9168 - acc: 0.4713 - val_loss: 0.9167 - val_acc: 0.6000\n",
      "Epoch 25/100\n",
      "2300/2300 [==============================] - 105s 46ms/step - loss: 0.9168 - acc: 0.4543 - val_loss: 0.9167 - val_acc: 0.6980\n",
      "Epoch 26/100\n",
      "2300/2300 [==============================] - 105s 46ms/step - loss: 0.9169 - acc: 0.4691 - val_loss: 0.9167 - val_acc: 0.6235\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 9.999999939225292e-10.\n",
      "Epoch 27/100\n",
      "2300/2300 [==============================] - 105s 46ms/step - loss: 0.9170 - acc: 0.4617 - val_loss: 0.9167 - val_acc: 0.4627\n",
      "Epoch 28/100\n",
      "2300/2300 [==============================] - 105s 46ms/step - loss: 0.9169 - acc: 0.4691 - val_loss: 0.9167 - val_acc: 0.5451\n",
      "Epoch 29/100\n",
      "2300/2300 [==============================] - 105s 46ms/step - loss: 0.9168 - acc: 0.4796 - val_loss: 0.9167 - val_acc: 0.5176\n",
      "Epoch 30/100\n",
      "2300/2300 [==============================] - 105s 46ms/step - loss: 0.9168 - acc: 0.4526 - val_loss: 0.9167 - val_acc: 0.5255\n",
      "Epoch 31/100\n",
      "2300/2300 [==============================] - 105s 46ms/step - loss: 0.9168 - acc: 0.4630 - val_loss: 0.9167 - val_acc: 0.5490\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.999999717180686e-11.\n",
      "Epoch 32/100\n",
      "2300/2300 [==============================] - 105s 46ms/step - loss: 0.9168 - acc: 0.4648 - val_loss: 0.9167 - val_acc: 0.3843\n",
      "Epoch 33/100\n",
      "2300/2300 [==============================] - 105s 46ms/step - loss: 0.9169 - acc: 0.4752 - val_loss: 0.9167 - val_acc: 0.6039\n",
      "Epoch 34/100\n",
      "2300/2300 [==============================] - 105s 46ms/step - loss: 0.9169 - acc: 0.4670 - val_loss: 0.9167 - val_acc: 0.6000\n",
      "Epoch 35/100\n",
      "2300/2300 [==============================] - 105s 46ms/step - loss: 0.9168 - acc: 0.4678 - val_loss: 0.9167 - val_acc: 0.5137\n",
      "Epoch 36/100\n",
      "2300/2300 [==============================] - 105s 46ms/step - loss: 0.9169 - acc: 0.4678 - val_loss: 0.9167 - val_acc: 0.5451\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 9.99999943962493e-12.\n",
      "Epoch 37/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2300/2300 [==============================] - 105s 46ms/step - loss: 0.9168 - acc: 0.4757 - val_loss: 0.9167 - val_acc: 0.5412\n",
      "Epoch 38/100\n",
      "2300/2300 [==============================] - 105s 46ms/step - loss: 0.9168 - acc: 0.4704 - val_loss: 0.9167 - val_acc: 0.5647\n",
      "Epoch 39/100\n",
      "2300/2300 [==============================] - 105s 46ms/step - loss: 0.9168 - acc: 0.4674 - val_loss: 0.9167 - val_acc: 0.5922\n",
      "Epoch 40/100\n",
      "2300/2300 [==============================] - 105s 46ms/step - loss: 0.9169 - acc: 0.4683 - val_loss: 0.9167 - val_acc: 0.4471\n",
      "Epoch 41/100\n",
      "2300/2300 [==============================] - 105s 46ms/step - loss: 0.9168 - acc: 0.4574 - val_loss: 0.9167 - val_acc: 0.5333\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.999999092680235e-13.\n",
      "Epoch 42/100\n",
      "2300/2300 [==============================] - 105s 45ms/step - loss: 0.9170 - acc: 0.4570 - val_loss: 0.9167 - val_acc: 0.6275\n",
      "Epoch 43/100\n",
      "2300/2300 [==============================] - 105s 46ms/step - loss: 0.9169 - acc: 0.4626 - val_loss: 0.9167 - val_acc: 0.5529\n",
      "Epoch 44/100\n",
      "2300/2300 [==============================] - 105s 46ms/step - loss: 0.9168 - acc: 0.4443 - val_loss: 0.9167 - val_acc: 0.5922\n",
      "Epoch 45/100\n",
      "2300/2300 [==============================] - 105s 46ms/step - loss: 0.9170 - acc: 0.4739 - val_loss: 0.9167 - val_acc: 0.6118\n",
      "Epoch 46/100\n",
      "2300/2300 [==============================] - 105s 46ms/step - loss: 0.9168 - acc: 0.4700 - val_loss: 0.9167 - val_acc: 0.3686\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 9.9999988758398e-14.\n",
      "Epoch 47/100\n",
      "2300/2300 [==============================] - 105s 45ms/step - loss: 0.9169 - acc: 0.4709 - val_loss: 0.9167 - val_acc: 0.5804\n",
      "Epoch 48/100\n",
      "2300/2300 [==============================] - 105s 46ms/step - loss: 0.9168 - acc: 0.4809 - val_loss: 0.9167 - val_acc: 0.4000\n",
      "Epoch 49/100\n",
      "2300/2300 [==============================] - 105s 46ms/step - loss: 0.9168 - acc: 0.4643 - val_loss: 0.9167 - val_acc: 0.5529\n",
      "Epoch 50/100\n",
      "2300/2300 [==============================] - 105s 46ms/step - loss: 0.9168 - acc: 0.4583 - val_loss: 0.9167 - val_acc: 0.6157\n",
      "Epoch 51/100\n",
      "2300/2300 [==============================] - 105s 46ms/step - loss: 0.9170 - acc: 0.4570 - val_loss: 0.9167 - val_acc: 0.5961\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 9.999999146890344e-15.\n",
      "Epoch 52/100\n",
      "2300/2300 [==============================] - 105s 46ms/step - loss: 0.9168 - acc: 0.4657 - val_loss: 0.9167 - val_acc: 0.5882\n",
      "Epoch 53/100\n",
      "2300/2300 [==============================] - 105s 46ms/step - loss: 0.9168 - acc: 0.4713 - val_loss: 0.9167 - val_acc: 0.4784\n",
      "Epoch 54/100\n",
      "2300/2300 [==============================] - 105s 46ms/step - loss: 0.9168 - acc: 0.4630 - val_loss: 0.9167 - val_acc: 0.4667\n",
      "Epoch 55/100\n",
      "2300/2300 [==============================] - 105s 46ms/step - loss: 0.9168 - acc: 0.4730 - val_loss: 0.9167 - val_acc: 0.5373\n",
      "Epoch 56/100\n",
      "2300/2300 [==============================] - 105s 46ms/step - loss: 0.9169 - acc: 0.4700 - val_loss: 0.9167 - val_acc: 0.5294\n",
      "\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 9.999998977483753e-16.\n",
      "Epoch 57/100\n",
      "2300/2300 [==============================] - 105s 46ms/step - loss: 0.9168 - acc: 0.4743 - val_loss: 0.9167 - val_acc: 0.6157\n",
      "Epoch 58/100\n",
      "2300/2300 [==============================] - 105s 46ms/step - loss: 0.9168 - acc: 0.4652 - val_loss: 0.9167 - val_acc: 0.5686\n",
      "Epoch 59/100\n",
      "2300/2300 [==============================] - 105s 46ms/step - loss: 0.9169 - acc: 0.4583 - val_loss: 0.9167 - val_acc: 0.4275\n",
      "Epoch 60/100\n",
      "2300/2300 [==============================] - 105s 46ms/step - loss: 0.9168 - acc: 0.4730 - val_loss: 0.9167 - val_acc: 0.6039\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-80ab8aba1340>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m model.fit(\n\u001b[1;32m     50\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     callbacks=callbacks)\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/bnns/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/virtualenvs/bnns/lib/python3.5/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/bnns/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/bnns/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/bnns/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "\n",
    "train_idx = folds[i][0]\n",
    "test_idx = folds[i][1]\n",
    "\n",
    "X_train, y_train = X[train_idx], y[train_idx]\n",
    "X_test, y_test = X[test_idx], y[test_idx]\n",
    "\n",
    "X_train = feature_extraction(X_train)\n",
    "X_test = feature_extraction(X_test)\n",
    "\n",
    "X_train = X_train.reshape(-1,1001,1)\n",
    "X_test = X_test.reshape(-1,1001,1)\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "\n",
    "# Create Model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(\n",
    "    Conv1D(\n",
    "        kernel_size=10,\n",
    "        filters=64,\n",
    "        padding='same',\n",
    "        input_shape=(1001,1)))\n",
    "model.add(BatchNormalization(momentum=0.1, epsilon=0.0001))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(\n",
    "        kernel_size=10, filters=128, padding='same'))\n",
    "model.add(BatchNormalization(momentum=0.1, epsilon=0.0001))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv1D(\n",
    "        kernel_size=10, filters=128, padding='same'))\n",
    "model.add(BatchNormalization(momentum=0.1, epsilon=0.0001))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(12, use_bias=False))\n",
    "model.add(BatchNormalization(momentum=0.1, epsilon=0.0001))\n",
    "\n",
    "# Train\n",
    "adam = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "model.compile(loss=squared_hinge, optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "model.fit(\n",
    "    X_train, y_train, batch_size=50, epochs=100, validation_data=(X_test,y_test),\n",
    "    callbacks=callbacks)\n",
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.classification import accuracy_score, recall_score, f1_score\n",
    "import scipy.stats as st\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../quantnn-keras/layers/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.losses import squared_hinge\n",
    "from keras.layers import Flatten, Dense, Activation, BatchNormalization, Conv1D\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "import numpy as np\n",
    "\n",
    "from binary_layers import BinaryConv2D, BinaryConv1D\n",
    "from binary_ops import binary_tanh as binary_tanh_op\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet Template 2017 data/LOSO/MHEALTH.npz\n",
      "255/255 [==============================] - 0s 413us/step\n",
      "[0.9179191189653734, 0.7960784323075238]\n",
      "263/263 [==============================] - 0s 433us/step\n",
      "[0.921292200061305, 0.5779467682874702]\n",
      "251/251 [==============================] - 0s 461us/step\n",
      "[0.9293723239366751, 0.44223107522227373]\n",
      "266/266 [==============================] - 0s 332us/step\n",
      "[0.9186151646133652, 0.6315789478165763]\n",
      "265/265 [==============================] - 0s 460us/step\n",
      "[0.9184935306603054, 0.6867924528301886]\n",
      "264/264 [==============================] - 0s 472us/step\n",
      "[0.9182958512595205, 0.6856060606060606]\n",
      "253/253 [==============================] - 0s 428us/step\n",
      "[0.9245998352883833, 0.36363636304738495]\n",
      "239/239 [==============================] - 0s 371us/step\n",
      "[0.9211378686098873, 0.6861924688062907]\n",
      "251/251 [==============================] - 0s 437us/step\n",
      "[0.9332977623578562, 0.609561756075141]\n",
      "248/248 [==============================] - 0s 417us/step\n",
      "[0.9205247375272936, 0.600806450651538]\n"
     ]
    }
   ],
   "source": [
    "data_input_file = 'data/LOSO/MHEALTH.npz'\n",
    "\n",
    "tmp = np.load(data_input_file)\n",
    "X = tmp['X']\n",
    "y = tmp['y']\n",
    "folds = tmp['folds']\n",
    "\n",
    "n_class = y.shape[1]\n",
    "\n",
    "avg_acc = []\n",
    "avg_recall = []\n",
    "avg_f1 = []\n",
    "y = np.argmax(y, axis=1)\n",
    "\n",
    "print('ConvNet Template 2017 {}'.format(data_input_file))\n",
    "\n",
    "for i in range(0, len(folds)):\n",
    "    train_idx = folds[i][0]\n",
    "    test_idx = folds[i][1]\n",
    "\n",
    "    X_train, y_train = X[train_idx], y[train_idx]\n",
    "    X_test, y_test = X[test_idx], y[test_idx]\n",
    "    X_train = X_train.squeeze()\n",
    "    X_test = X_test.squeeze()\n",
    "    y_train = to_categorical(y_train)\n",
    "    y_test = to_categorical(y_test)\n",
    "    \n",
    "    # Create Model\n",
    "    model = Sequential()\n",
    "\n",
    "    _momentum = 0.1\n",
    "    _epsilon = 0.001\n",
    "    \n",
    "    model.add(\n",
    "        BinaryConv1D(\n",
    "            kernel_size=30,\n",
    "            filters=64,\n",
    "            padding='same',\n",
    "            input_shape=(250,23),\n",
    "            kernel_lr_multiplier=10,use_bias=False))\n",
    "    model.add(BatchNormalization(momentum=_momentum, epsilon=_epsilon))\n",
    "    model.add(Activation(binary_tanh_op))\n",
    "\n",
    "\n",
    "    model.add(\n",
    "        BinaryConv1D(\n",
    "            kernel_size=30, filters=128, padding='same',\n",
    "            kernel_lr_multiplier=10,use_bias=False))\n",
    "    model.add(BatchNormalization(momentum=_momentum, epsilon=_epsilon))\n",
    "    model.add(Activation(binary_tanh_op))\n",
    "\n",
    "\n",
    "    model.add(\n",
    "        BinaryConv1D(\n",
    "            kernel_size=30, filters=256, padding='same',\n",
    "            kernel_lr_multiplier=10,use_bias=False))\n",
    "    model.add(BatchNormalization(momentum=_momentum, epsilon=_epsilon))\n",
    "    model.add(Activation(binary_tanh_op))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(12, use_bias=False))\n",
    "    model.add(BatchNormalization(momentum=_momentum, epsilon=_epsilon))\n",
    "\n",
    "    # Train\n",
    "    adam = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    model.compile(loss=squared_hinge, optimizer=adam, metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "\n",
    "    model.fit(\n",
    "        X_train, y_train, batch_size=50, epochs=100, validation_data=(X_test,y_test), verbose=0 )\n",
    "    print (model.evaluate(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2307, 1, 250, 23) (248, 1, 250, 23)\n",
      "(2307,) (248,)\n"
     ]
    }
   ],
   "source": [
    "print (X_train.shape, X_test.shape)\n",
    "print (y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.squeeze()\n",
    "X_test = X_test.squeeze()\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2307, 250, 23) (248, 250, 23)\n",
      "(2307, 12) (248, 12)\n"
     ]
    }
   ],
   "source": [
    "print (X_train.shape, X_test.shape)\n",
    "print (y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 2) 10\n"
     ]
    }
   ],
   "source": [
    "print (folds.shape, len(folds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "binary_conv1d_37 (BinaryConv (None, 250, 64)           44160     \n",
      "_________________________________________________________________\n",
      "batch_normalization_61 (Batc (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "binary_conv1d_38 (BinaryConv (None, 250, 128)          245760    \n",
      "_________________________________________________________________\n",
      "batch_normalization_62 (Batc (None, 250, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "binary_conv1d_39 (BinaryConv (None, 250, 256)          983040    \n",
      "_________________________________________________________________\n",
      "batch_normalization_63 (Batc (None, 250, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 250, 256)          0         \n",
      "_________________________________________________________________\n",
      "flatten_20 (Flatten)         (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 12)                768000    \n",
      "_________________________________________________________________\n",
      "batch_normalization_64 (Batc (None, 12)                48        \n",
      "=================================================================\n",
      "Total params: 2,042,800\n",
      "Trainable params: 2,041,880\n",
      "Non-trainable params: 920\n",
      "_________________________________________________________________\n",
      "248/248 [==============================] - 0s 519us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9192171500575158, 0.6209677428968491]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(\n",
    "    BinaryConv1D(\n",
    "        kernel_size=30,\n",
    "        filters=64,\n",
    "        padding='same',\n",
    "        input_shape=(250,23),\n",
    "        kernel_lr_multiplier=10,use_bias=False))\n",
    "model.add(BatchNormalization(momentum=0.1, epsilon=0.0001))\n",
    "model.add(Activation(binary_tanh_op))\n",
    "\n",
    "\n",
    "model.add(\n",
    "    BinaryConv1D(\n",
    "        kernel_size=30, filters=128, padding='same',\n",
    "        kernel_lr_multiplier=10,use_bias=False))\n",
    "model.add(BatchNormalization(momentum=0.1, epsilon=0.0001))\n",
    "model.add(Activation(binary_tanh_op))\n",
    "\n",
    "\n",
    "model.add(\n",
    "    BinaryConv1D(\n",
    "        kernel_size=30, filters=256, padding='same',\n",
    "        kernel_lr_multiplier=10,use_bias=False))\n",
    "model.add(BatchNormalization(momentum=0.1, epsilon=0.0001))\n",
    "model.add(Activation(binary_tanh_op))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(12, use_bias=False))\n",
    "model.add(BatchNormalization(momentum=0.1, epsilon=0.0001))\n",
    "\n",
    "# Train\n",
    "adam = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "model.compile(loss=squared_hinge, optimizer=adam, metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train, batch_size=50, epochs=100, validation_data=(X_test,y_test), verbose=0 )\n",
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_8 (Conv1D)            (None, 250, 64)           14784     \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 250, 64)           41024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 12)                192000    \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 12)                48        \n",
      "=================================================================\n",
      "Total params: 248,368\n",
      "Trainable params: 248,088\n",
      "Non-trainable params: 280\n",
      "_________________________________________________________________\n",
      "Train on 2307 samples, validate on 248 samples\n",
      "Epoch 1/100\n",
      "2307/2307 [==============================] - 2s 925us/step - loss: 0.9344 - acc: 0.4538 - val_loss: 0.9257 - val_acc: 0.5363\n",
      "Epoch 2/100\n",
      "2307/2307 [==============================] - 0s 160us/step - loss: 0.9192 - acc: 0.6407 - val_loss: 0.9233 - val_acc: 0.4597\n",
      "Epoch 3/100\n",
      "2307/2307 [==============================] - 0s 156us/step - loss: 0.9174 - acc: 0.7018 - val_loss: 0.9185 - val_acc: 0.6048\n",
      "Epoch 4/100\n",
      "2307/2307 [==============================] - 0s 155us/step - loss: 0.9169 - acc: 0.7235 - val_loss: 0.9189 - val_acc: 0.7460\n",
      "Epoch 5/100\n",
      "2307/2307 [==============================] - 0s 153us/step - loss: 0.9168 - acc: 0.7334 - val_loss: 0.9187 - val_acc: 0.5847\n",
      "Epoch 6/100\n",
      "2307/2307 [==============================] - 0s 153us/step - loss: 0.9168 - acc: 0.7425 - val_loss: 0.9178 - val_acc: 0.6694\n",
      "Epoch 7/100\n",
      "2307/2307 [==============================] - 0s 152us/step - loss: 0.9168 - acc: 0.7369 - val_loss: 0.9298 - val_acc: 0.4032\n",
      "Epoch 8/100\n",
      "2307/2307 [==============================] - 0s 156us/step - loss: 0.9169 - acc: 0.6983 - val_loss: 0.9191 - val_acc: 0.5645\n",
      "Epoch 9/100\n",
      "2307/2307 [==============================] - 0s 182us/step - loss: 0.9169 - acc: 0.7295 - val_loss: 0.9180 - val_acc: 0.6331\n",
      "Epoch 10/100\n",
      "2307/2307 [==============================] - 0s 154us/step - loss: 0.9167 - acc: 0.7464 - val_loss: 0.9207 - val_acc: 0.4839\n",
      "Epoch 11/100\n",
      "2307/2307 [==============================] - 0s 157us/step - loss: 0.9167 - acc: 0.7460 - val_loss: 0.9180 - val_acc: 0.7137\n",
      "Epoch 12/100\n",
      "2307/2307 [==============================] - 0s 152us/step - loss: 0.9167 - acc: 0.7525 - val_loss: 0.9220 - val_acc: 0.5081\n",
      "Epoch 13/100\n",
      "2307/2307 [==============================] - 0s 155us/step - loss: 0.9167 - acc: 0.7503 - val_loss: 0.9227 - val_acc: 0.5081\n",
      "Epoch 14/100\n",
      "2307/2307 [==============================] - 0s 174us/step - loss: 0.9167 - acc: 0.7573 - val_loss: 0.9235 - val_acc: 0.5645\n",
      "Epoch 15/100\n",
      "2307/2307 [==============================] - 0s 151us/step - loss: 0.9167 - acc: 0.7729 - val_loss: 0.9196 - val_acc: 0.6976\n",
      "Epoch 16/100\n",
      "2307/2307 [==============================] - 0s 175us/step - loss: 0.9167 - acc: 0.7815 - val_loss: 0.9216 - val_acc: 0.4718\n",
      "Epoch 17/100\n",
      "2307/2307 [==============================] - 0s 157us/step - loss: 0.9167 - acc: 0.7729 - val_loss: 0.9243 - val_acc: 0.6250\n",
      "Epoch 18/100\n",
      "2307/2307 [==============================] - 0s 149us/step - loss: 0.9167 - acc: 0.7859 - val_loss: 0.9173 - val_acc: 0.5847\n",
      "Epoch 19/100\n",
      "2307/2307 [==============================] - 0s 155us/step - loss: 0.9168 - acc: 0.7711 - val_loss: 0.9213 - val_acc: 0.5121\n",
      "Epoch 20/100\n",
      "2307/2307 [==============================] - 0s 152us/step - loss: 0.9167 - acc: 0.7655 - val_loss: 0.9191 - val_acc: 0.5766\n",
      "Epoch 21/100\n",
      "2307/2307 [==============================] - 0s 166us/step - loss: 0.9167 - acc: 0.7750 - val_loss: 0.9211 - val_acc: 0.4758\n",
      "Epoch 22/100\n",
      "2307/2307 [==============================] - 0s 174us/step - loss: 0.9167 - acc: 0.7919 - val_loss: 0.9178 - val_acc: 0.5605\n",
      "Epoch 23/100\n",
      "2307/2307 [==============================] - 0s 155us/step - loss: 0.9167 - acc: 0.7937 - val_loss: 0.9191 - val_acc: 0.6613\n",
      "Epoch 24/100\n",
      "2307/2307 [==============================] - 0s 156us/step - loss: 0.9167 - acc: 0.7945 - val_loss: 0.9226 - val_acc: 0.5081\n",
      "Epoch 25/100\n",
      "2307/2307 [==============================] - 0s 157us/step - loss: 0.9167 - acc: 0.8028 - val_loss: 0.9189 - val_acc: 0.5202\n",
      "Epoch 26/100\n",
      "2307/2307 [==============================] - 0s 175us/step - loss: 0.9167 - acc: 0.7724 - val_loss: 0.9174 - val_acc: 0.4839\n",
      "Epoch 27/100\n",
      "2307/2307 [==============================] - 0s 162us/step - loss: 0.9167 - acc: 0.7989 - val_loss: 0.9186 - val_acc: 0.5726\n",
      "Epoch 28/100\n",
      "2307/2307 [==============================] - 0s 160us/step - loss: 0.9167 - acc: 0.8097 - val_loss: 0.9193 - val_acc: 0.6008\n",
      "Epoch 29/100\n",
      "2307/2307 [==============================] - 0s 158us/step - loss: 0.9167 - acc: 0.8058 - val_loss: 0.9188 - val_acc: 0.5968\n",
      "Epoch 30/100\n",
      "2307/2307 [==============================] - 0s 156us/step - loss: 0.9167 - acc: 0.8166 - val_loss: 0.9214 - val_acc: 0.5887\n",
      "Epoch 31/100\n",
      "2307/2307 [==============================] - 0s 166us/step - loss: 0.9167 - acc: 0.8353 - val_loss: 0.9196 - val_acc: 0.5847\n",
      "Epoch 32/100\n",
      "2307/2307 [==============================] - 0s 159us/step - loss: 0.9167 - acc: 0.8136 - val_loss: 0.9186 - val_acc: 0.3508\n",
      "Epoch 33/100\n",
      "2307/2307 [==============================] - 0s 162us/step - loss: 0.9167 - acc: 0.8062 - val_loss: 0.9176 - val_acc: 0.6048\n",
      "Epoch 34/100\n",
      "2307/2307 [==============================] - 0s 169us/step - loss: 0.9167 - acc: 0.8375 - val_loss: 0.9233 - val_acc: 0.6089\n",
      "Epoch 35/100\n",
      "2307/2307 [==============================] - 0s 159us/step - loss: 0.9167 - acc: 0.8336 - val_loss: 0.9229 - val_acc: 0.6411\n",
      "Epoch 36/100\n",
      "2307/2307 [==============================] - 0s 155us/step - loss: 0.9167 - acc: 0.8249 - val_loss: 0.9185 - val_acc: 0.4556\n",
      "Epoch 37/100\n",
      "2307/2307 [==============================] - 0s 162us/step - loss: 0.9167 - acc: 0.8283 - val_loss: 0.9175 - val_acc: 0.6492\n",
      "Epoch 38/100\n",
      "2307/2307 [==============================] - 0s 163us/step - loss: 0.9167 - acc: 0.8279 - val_loss: 0.9184 - val_acc: 0.6895\n",
      "Epoch 39/100\n",
      "2307/2307 [==============================] - 0s 166us/step - loss: 0.9167 - acc: 0.8270 - val_loss: 0.9180 - val_acc: 0.6895\n",
      "Epoch 40/100\n",
      "2307/2307 [==============================] - 0s 161us/step - loss: 0.9167 - acc: 0.8201 - val_loss: 0.9180 - val_acc: 0.7823\n",
      "Epoch 41/100\n",
      "2307/2307 [==============================] - 0s 161us/step - loss: 0.9167 - acc: 0.8353 - val_loss: 0.9198 - val_acc: 0.7581\n",
      "Epoch 42/100\n",
      "2307/2307 [==============================] - 0s 166us/step - loss: 0.9167 - acc: 0.8253 - val_loss: 0.9233 - val_acc: 0.6210\n",
      "Epoch 43/100\n",
      "2307/2307 [==============================] - 0s 160us/step - loss: 0.9167 - acc: 0.8305 - val_loss: 0.9175 - val_acc: 0.7379\n",
      "Epoch 44/100\n",
      "2307/2307 [==============================] - 0s 159us/step - loss: 0.9167 - acc: 0.8405 - val_loss: 0.9172 - val_acc: 0.7298\n",
      "Epoch 45/100\n",
      "2307/2307 [==============================] - 0s 156us/step - loss: 0.9167 - acc: 0.8457 - val_loss: 0.9191 - val_acc: 0.7621\n",
      "Epoch 46/100\n",
      "2307/2307 [==============================] - 0s 163us/step - loss: 0.9167 - acc: 0.8344 - val_loss: 0.9175 - val_acc: 0.7944\n",
      "Epoch 47/100\n",
      "2307/2307 [==============================] - 0s 161us/step - loss: 0.9167 - acc: 0.8331 - val_loss: 0.9174 - val_acc: 0.6452\n",
      "Epoch 48/100\n",
      "2307/2307 [==============================] - 0s 166us/step - loss: 0.9167 - acc: 0.8448 - val_loss: 0.9207 - val_acc: 0.6815\n",
      "Epoch 49/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2307/2307 [==============================] - 0s 152us/step - loss: 0.9167 - acc: 0.8479 - val_loss: 0.9177 - val_acc: 0.8024\n",
      "Epoch 50/100\n",
      "2307/2307 [==============================] - 0s 145us/step - loss: 0.9167 - acc: 0.8457 - val_loss: 0.9180 - val_acc: 0.6089\n",
      "Epoch 51/100\n",
      "2307/2307 [==============================] - 0s 158us/step - loss: 0.9167 - acc: 0.8539 - val_loss: 0.9177 - val_acc: 0.7702\n",
      "Epoch 52/100\n",
      "2307/2307 [==============================] - 0s 145us/step - loss: 0.9167 - acc: 0.8422 - val_loss: 0.9195 - val_acc: 0.6774\n",
      "Epoch 53/100\n",
      "2307/2307 [==============================] - 0s 152us/step - loss: 0.9167 - acc: 0.8305 - val_loss: 0.9173 - val_acc: 0.6250\n",
      "Epoch 54/100\n",
      "2307/2307 [==============================] - 0s 155us/step - loss: 0.9167 - acc: 0.8405 - val_loss: 0.9175 - val_acc: 0.7258\n",
      "Epoch 55/100\n",
      "2307/2307 [==============================] - 0s 151us/step - loss: 0.9167 - acc: 0.8483 - val_loss: 0.9176 - val_acc: 0.8629\n",
      "Epoch 56/100\n",
      "2307/2307 [==============================] - 0s 150us/step - loss: 0.9167 - acc: 0.8457 - val_loss: 0.9181 - val_acc: 0.6815\n",
      "Epoch 57/100\n",
      "2307/2307 [==============================] - 0s 148us/step - loss: 0.9167 - acc: 0.8197 - val_loss: 0.9177 - val_acc: 0.7823\n",
      "Epoch 58/100\n",
      "2307/2307 [==============================] - 0s 160us/step - loss: 0.9167 - acc: 0.8257 - val_loss: 0.9222 - val_acc: 0.4919\n",
      "Epoch 59/100\n",
      "2307/2307 [==============================] - 0s 152us/step - loss: 0.9167 - acc: 0.8283 - val_loss: 0.9195 - val_acc: 0.5766\n",
      "Epoch 60/100\n",
      "2307/2307 [==============================] - 0s 156us/step - loss: 0.9167 - acc: 0.8305 - val_loss: 0.9175 - val_acc: 0.7702\n",
      "Epoch 61/100\n",
      "2307/2307 [==============================] - 0s 154us/step - loss: 0.9167 - acc: 0.8409 - val_loss: 0.9183 - val_acc: 0.7581\n",
      "Epoch 62/100\n",
      "2307/2307 [==============================] - 0s 149us/step - loss: 0.9167 - acc: 0.8249 - val_loss: 0.9178 - val_acc: 0.5806\n",
      "Epoch 63/100\n",
      "2307/2307 [==============================] - 0s 153us/step - loss: 0.9167 - acc: 0.8275 - val_loss: 0.9303 - val_acc: 0.6008\n",
      "Epoch 64/100\n",
      "2307/2307 [==============================] - 0s 155us/step - loss: 0.9167 - acc: 0.8479 - val_loss: 0.9183 - val_acc: 0.5685\n",
      "Epoch 65/100\n",
      "2307/2307 [==============================] - 0s 161us/step - loss: 0.9167 - acc: 0.8565 - val_loss: 0.9211 - val_acc: 0.6573\n",
      "Epoch 66/100\n",
      "2307/2307 [==============================] - 0s 156us/step - loss: 0.9167 - acc: 0.8630 - val_loss: 0.9181 - val_acc: 0.5968\n",
      "Epoch 67/100\n",
      "2307/2307 [==============================] - 0s 155us/step - loss: 0.9167 - acc: 0.8548 - val_loss: 0.9171 - val_acc: 0.7540\n",
      "Epoch 68/100\n",
      "2307/2307 [==============================] - 0s 153us/step - loss: 0.9167 - acc: 0.8522 - val_loss: 0.9184 - val_acc: 0.6573\n",
      "Epoch 69/100\n",
      "2307/2307 [==============================] - 0s 160us/step - loss: 0.9167 - acc: 0.8513 - val_loss: 0.9167 - val_acc: 0.6290\n",
      "Epoch 70/100\n",
      "2307/2307 [==============================] - 0s 158us/step - loss: 0.9167 - acc: 0.8630 - val_loss: 0.9173 - val_acc: 0.7863\n",
      "Epoch 71/100\n",
      "2307/2307 [==============================] - 0s 158us/step - loss: 0.9167 - acc: 0.8587 - val_loss: 0.9173 - val_acc: 0.6250\n",
      "Epoch 72/100\n",
      "2307/2307 [==============================] - 0s 156us/step - loss: 0.9167 - acc: 0.8721 - val_loss: 0.9171 - val_acc: 0.6815\n",
      "Epoch 73/100\n",
      "2307/2307 [==============================] - 0s 160us/step - loss: 0.9167 - acc: 0.8301 - val_loss: 0.9168 - val_acc: 0.5847\n",
      "Epoch 74/100\n",
      "2307/2307 [==============================] - 0s 163us/step - loss: 0.9167 - acc: 0.8240 - val_loss: 0.9194 - val_acc: 0.6452\n",
      "Epoch 75/100\n",
      "2307/2307 [==============================] - 0s 161us/step - loss: 0.9167 - acc: 0.8175 - val_loss: 0.9175 - val_acc: 0.6371\n",
      "Epoch 76/100\n",
      "2307/2307 [==============================] - 0s 160us/step - loss: 0.9167 - acc: 0.8153 - val_loss: 0.9173 - val_acc: 0.7177\n",
      "Epoch 77/100\n",
      "2307/2307 [==============================] - 0s 160us/step - loss: 0.9167 - acc: 0.8314 - val_loss: 0.9170 - val_acc: 0.7339\n",
      "Epoch 78/100\n",
      "2307/2307 [==============================] - 0s 158us/step - loss: 0.9167 - acc: 0.8388 - val_loss: 0.9168 - val_acc: 0.7581\n",
      "Epoch 79/100\n",
      "2307/2307 [==============================] - 0s 155us/step - loss: 0.9167 - acc: 0.8279 - val_loss: 0.9171 - val_acc: 0.7782\n",
      "Epoch 80/100\n",
      "2307/2307 [==============================] - 0s 165us/step - loss: 0.9167 - acc: 0.8349 - val_loss: 0.9202 - val_acc: 0.3952\n",
      "Epoch 81/100\n",
      "2307/2307 [==============================] - 0s 156us/step - loss: 0.9167 - acc: 0.8344 - val_loss: 0.9171 - val_acc: 0.7944\n",
      "Epoch 82/100\n",
      "2307/2307 [==============================] - 0s 149us/step - loss: 0.9167 - acc: 0.8431 - val_loss: 0.9186 - val_acc: 0.6613\n",
      "Epoch 83/100\n",
      "2307/2307 [==============================] - 0s 163us/step - loss: 0.9167 - acc: 0.8444 - val_loss: 0.9171 - val_acc: 0.5887\n",
      "Epoch 84/100\n",
      "2307/2307 [==============================] - 0s 156us/step - loss: 0.9167 - acc: 0.8487 - val_loss: 0.9173 - val_acc: 0.7782\n",
      "Epoch 85/100\n",
      "2307/2307 [==============================] - 0s 181us/step - loss: 0.9167 - acc: 0.8574 - val_loss: 0.9174 - val_acc: 0.7621\n",
      "Epoch 86/100\n",
      "2307/2307 [==============================] - 0s 157us/step - loss: 0.9167 - acc: 0.8301 - val_loss: 0.9169 - val_acc: 0.6774\n",
      "Epoch 87/100\n",
      "2307/2307 [==============================] - 0s 158us/step - loss: 0.9167 - acc: 0.8466 - val_loss: 0.9173 - val_acc: 0.4315\n",
      "Epoch 88/100\n",
      "2307/2307 [==============================] - 0s 158us/step - loss: 0.9167 - acc: 0.8518 - val_loss: 0.9171 - val_acc: 0.7258\n",
      "Epoch 89/100\n",
      "2307/2307 [==============================] - 0s 162us/step - loss: 0.9167 - acc: 0.8292 - val_loss: 0.9169 - val_acc: 0.7016\n",
      "Epoch 90/100\n",
      "2307/2307 [==============================] - 0s 155us/step - loss: 0.9167 - acc: 0.8240 - val_loss: 0.9168 - val_acc: 0.7903\n",
      "Epoch 91/100\n",
      "2307/2307 [==============================] - 0s 148us/step - loss: 0.9167 - acc: 0.8197 - val_loss: 0.9167 - val_acc: 0.6250\n",
      "Epoch 92/100\n",
      "2307/2307 [==============================] - 0s 156us/step - loss: 0.9167 - acc: 0.8210 - val_loss: 0.9169 - val_acc: 0.7944\n",
      "Epoch 93/100\n",
      "2307/2307 [==============================] - 0s 158us/step - loss: 0.9167 - acc: 0.8257 - val_loss: 0.9167 - val_acc: 0.7581\n",
      "Epoch 94/100\n",
      "2307/2307 [==============================] - 0s 157us/step - loss: 0.9167 - acc: 0.8292 - val_loss: 0.9171 - val_acc: 0.7742\n",
      "Epoch 95/100\n",
      "2307/2307 [==============================] - 0s 161us/step - loss: 0.9167 - acc: 0.8327 - val_loss: 0.9176 - val_acc: 0.7823\n",
      "Epoch 96/100\n",
      "2307/2307 [==============================] - 0s 153us/step - loss: 0.9167 - acc: 0.8149 - val_loss: 0.9197 - val_acc: 0.6008\n",
      "Epoch 97/100\n",
      "2307/2307 [==============================] - 0s 153us/step - loss: 0.9167 - acc: 0.8362 - val_loss: 0.9195 - val_acc: 0.6653\n",
      "Epoch 98/100\n",
      "2307/2307 [==============================] - 0s 163us/step - loss: 0.9167 - acc: 0.8292 - val_loss: 0.9177 - val_acc: 0.5927\n",
      "Epoch 99/100\n",
      "2307/2307 [==============================] - 0s 164us/step - loss: 0.9167 - acc: 0.8292 - val_loss: 0.9170 - val_acc: 0.7944\n",
      "Epoch 100/100\n",
      "2307/2307 [==============================] - 0s 160us/step - loss: 0.9167 - acc: 0.8249 - val_loss: 0.9168 - val_acc: 0.6653\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4f88746fd0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(\n",
    "    Conv1D(\n",
    "        kernel_size=10,\n",
    "        filters=64,\n",
    "        padding='same',\n",
    "        input_shape=(250,23)))\n",
    "model.add(BatchNormalization(momentum=0.1, epsilon=0.0001))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(\n",
    "        kernel_size=10, filters=64, padding='same'))\n",
    "model.add(BatchNormalization(momentum=0.1, epsilon=0.0001))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(12, use_bias=False))\n",
    "model.add(BatchNormalization(momentum=0.1, epsilon=0.0001))\n",
    "\n",
    "# Train\n",
    "adam = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "model.compile(loss=squared_hinge, optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "model.fit(\n",
    "    X_train, y_train, batch_size=50, epochs=100, validation_data=(X_test,y_test))\n",
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248/248 [==============================] - 0s 191us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9168156596922106, 0.6653225806451613]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

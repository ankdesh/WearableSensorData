{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.classification import accuracy_score, recall_score, f1_score\n",
    "import scipy.stats as st\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../quantnn-keras/layers/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.losses import squared_hinge\n",
    "from keras.layers import Flatten, Dense, Activation, BatchNormalization, Conv1D\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "import numpy as np\n",
    "\n",
    "from binary_layers import BinaryConv2D, BinaryConv1D\n",
    "from binary_ops import binary_tanh as binary_tanh_op\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet Template 2017 data/LOSO/MHEALTH.npz\n",
      "255/255 [==============================] - 0s 413us/step\n",
      "[0.9179191189653734, 0.7960784323075238]\n",
      "263/263 [==============================] - 0s 433us/step\n",
      "[0.921292200061305, 0.5779467682874702]\n",
      "251/251 [==============================] - 0s 461us/step\n",
      "[0.9293723239366751, 0.44223107522227373]\n",
      "266/266 [==============================] - 0s 332us/step\n",
      "[0.9186151646133652, 0.6315789478165763]\n",
      "265/265 [==============================] - 0s 460us/step\n",
      "[0.9184935306603054, 0.6867924528301886]\n",
      "264/264 [==============================] - 0s 472us/step\n",
      "[0.9182958512595205, 0.6856060606060606]\n",
      "253/253 [==============================] - 0s 428us/step\n",
      "[0.9245998352883833, 0.36363636304738495]\n",
      "239/239 [==============================] - 0s 371us/step\n",
      "[0.9211378686098873, 0.6861924688062907]\n",
      "251/251 [==============================] - 0s 437us/step\n",
      "[0.9332977623578562, 0.609561756075141]\n",
      "248/248 [==============================] - 0s 417us/step\n",
      "[0.9205247375272936, 0.600806450651538]\n"
     ]
    }
   ],
   "source": [
    "data_input_file = 'data/LOSO/WHARF.npz'\n",
    "\n",
    "tmp = np.load(data_input_file)\n",
    "X = tmp['X']\n",
    "y = tmp['y']\n",
    "folds = tmp['folds']\n",
    "\n",
    "n_class = y.shape[1]\n",
    "\n",
    "avg_acc = []\n",
    "avg_recall = []\n",
    "avg_f1 = []\n",
    "y = np.argmax(y, axis=1)\n",
    "\n",
    "print('ConvNet Template 2017 {}'.format(data_input_file))\n",
    "\n",
    "for i in range(0, len(folds)):\n",
    "    train_idx = folds[i][0]\n",
    "    test_idx = folds[i][1]\n",
    "\n",
    "    X_train, y_train = X[train_idx], y[train_idx]\n",
    "    X_test, y_test = X[test_idx], y[test_idx]\n",
    "    X_train = X_train.squeeze()\n",
    "    X_test = X_test.squeeze()\n",
    "    y_train = to_categorical(y_train)\n",
    "    y_test = to_categorical(y_test)\n",
    "    \n",
    "    # Create Model\n",
    "    model = Sequential()\n",
    "\n",
    "    _momentum = 0.1\n",
    "    _epsilon = 0.001\n",
    "    \n",
    "    model.add(\n",
    "        BinaryConv1D(\n",
    "            kernel_size=30,\n",
    "            filters=64,\n",
    "            padding='same',\n",
    "            input_shape=(250,23),\n",
    "            kernel_lr_multiplier=10,use_bias=False))\n",
    "    model.add(BatchNormalization(momentum=_momentum, epsilon=_epsilon))\n",
    "    model.add(Activation(binary_tanh_op))\n",
    "\n",
    "\n",
    "    model.add(\n",
    "        BinaryConv1D(\n",
    "            kernel_size=30, filters=128, padding='same',\n",
    "            kernel_lr_multiplier=10,use_bias=False))\n",
    "    model.add(BatchNormalization(momentum=_momentum, epsilon=_epsilon))\n",
    "    model.add(Activation(binary_tanh_op))\n",
    "\n",
    "\n",
    "    model.add(\n",
    "        BinaryConv1D(\n",
    "            kernel_size=30, filters=256, padding='same',\n",
    "            kernel_lr_multiplier=10,use_bias=False))\n",
    "    model.add(BatchNormalization(momentum=_momentum, epsilon=_epsilon))\n",
    "    model.add(Activation(binary_tanh_op))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(12, use_bias=False))\n",
    "    model.add(BatchNormalization(momentum=_momentum, epsilon=_epsilon))\n",
    "\n",
    "    # Train\n",
    "    adam = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    model.compile(loss=squared_hinge, optimizer=adam, metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "\n",
    "    model.fit(\n",
    "        X_train, y_train, batch_size=50, epochs=100, validation_data=(X_test,y_test), verbose=0 )\n",
    "    print (model.evaluate(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet Template 2017 data/LOSO/WHARF.npz\n",
      "(2189, 160, 3) (1682, 160, 3)\n",
      "(2189, 12) (1682, 12)\n"
     ]
    }
   ],
   "source": [
    "data_input_file = 'data/LOSO/WHARF.npz'\n",
    "\n",
    "tmp = np.load(data_input_file)\n",
    "X = tmp['X']\n",
    "y = tmp['y']\n",
    "folds = tmp['folds']\n",
    "\n",
    "n_class = y.shape[1]\n",
    "\n",
    "avg_acc = []\n",
    "avg_recall = []\n",
    "avg_f1 = []\n",
    "y = np.argmax(y, axis=1)\n",
    "\n",
    "print('ConvNet Template 2017 {}'.format(data_input_file))\n",
    "\n",
    "i = 0\n",
    "train_idx = folds[i][0]\n",
    "test_idx = folds[i][1]\n",
    "\n",
    "X_train, y_train = X[train_idx], y[train_idx]\n",
    "X_test, y_test = X[test_idx], y[test_idx]\n",
    "X_train = X_train.squeeze()\n",
    "X_test = X_test.squeeze()\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "print (X_train.shape, X_test.shape)\n",
    "print (y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 2) 16\n"
     ]
    }
   ],
   "source": [
    "print (folds.shape, len(folds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet Template 2017 data/LOSO/WHARF.npz\n",
      "WARNING:tensorflow:From /home/ankdesh/virtualenvs/bnns/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "binary_conv1d_1 (BinaryConv1 (None, 160, 64)           1920      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 160, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 160, 64)           0         \n",
      "_________________________________________________________________\n",
      "binary_conv1d_2 (BinaryConv1 (None, 160, 128)          81920     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 160, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 160, 128)          0         \n",
      "_________________________________________________________________\n",
      "binary_conv1d_3 (BinaryConv1 (None, 160, 128)          163840    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 160, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 160, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 20480)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 12)                245760    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 12)                48        \n",
      "=================================================================\n",
      "Total params: 494,768\n",
      "Trainable params: 494,104\n",
      "Non-trainable params: 664\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /home/ankdesh/virtualenvs/bnns/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 2189 samples, validate on 1682 samples\n",
      "Epoch 1/100\n",
      "2189/2189 [==============================] - 9s 4ms/step - loss: 0.9592 - acc: 0.1905 - val_loss: 0.9638 - val_acc: 0.1700\n",
      "Epoch 2/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9346 - acc: 0.2773 - val_loss: 0.9538 - val_acc: 0.2093\n",
      "Epoch 3/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9301 - acc: 0.3166 - val_loss: 0.9292 - val_acc: 0.2866\n",
      "Epoch 4/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9271 - acc: 0.3289 - val_loss: 0.9295 - val_acc: 0.3424\n",
      "Epoch 5/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9265 - acc: 0.3225 - val_loss: 0.9278 - val_acc: 0.3430\n",
      "Epoch 6/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9251 - acc: 0.3413 - val_loss: 0.9263 - val_acc: 0.3728\n",
      "Epoch 7/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9246 - acc: 0.3559 - val_loss: 0.9286 - val_acc: 0.2806\n",
      "Epoch 8/100\n",
      "2189/2189 [==============================] - 8s 3ms/step - loss: 0.9236 - acc: 0.3591 - val_loss: 0.9291 - val_acc: 0.3389\n",
      "Epoch 9/100\n",
      "2189/2189 [==============================] - 8s 3ms/step - loss: 0.9238 - acc: 0.3476 - val_loss: 0.9295 - val_acc: 0.3282\n",
      "Epoch 10/100\n",
      "2189/2189 [==============================] - 8s 3ms/step - loss: 0.9228 - acc: 0.3595 - val_loss: 0.9260 - val_acc: 0.3401\n",
      "Epoch 11/100\n",
      "2189/2189 [==============================] - 8s 3ms/step - loss: 0.9221 - acc: 0.3444 - val_loss: 0.9284 - val_acc: 0.2872\n",
      "Epoch 12/100\n",
      "2189/2189 [==============================] - 8s 3ms/step - loss: 0.9216 - acc: 0.3783 - val_loss: 0.9328 - val_acc: 0.3121\n",
      "Epoch 13/100\n",
      "2189/2189 [==============================] - 8s 3ms/step - loss: 0.9207 - acc: 0.3787 - val_loss: 0.9259 - val_acc: 0.3627\n",
      "Epoch 14/100\n",
      "2189/2189 [==============================] - 8s 3ms/step - loss: 0.9207 - acc: 0.3650 - val_loss: 0.9304 - val_acc: 0.2925\n",
      "Epoch 15/100\n",
      "2189/2189 [==============================] - 8s 3ms/step - loss: 0.9203 - acc: 0.3714 - val_loss: 0.9277 - val_acc: 0.3044\n",
      "Epoch 16/100\n",
      "2189/2189 [==============================] - 8s 3ms/step - loss: 0.9200 - acc: 0.3828 - val_loss: 0.9284 - val_acc: 0.3115\n",
      "Epoch 17/100\n",
      "2189/2189 [==============================] - 8s 3ms/step - loss: 0.9203 - acc: 0.3805 - val_loss: 0.9296 - val_acc: 0.3496\n",
      "Epoch 18/100\n",
      "2189/2189 [==============================] - 8s 3ms/step - loss: 0.9200 - acc: 0.3627 - val_loss: 0.9324 - val_acc: 0.3430\n",
      "Epoch 19/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9201 - acc: 0.3627 - val_loss: 0.9272 - val_acc: 0.3597\n",
      "Epoch 20/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9202 - acc: 0.3723 - val_loss: 0.9382 - val_acc: 0.2551\n",
      "Epoch 21/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9199 - acc: 0.3646 - val_loss: 0.9373 - val_acc: 0.2010\n",
      "Epoch 22/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9194 - acc: 0.3878 - val_loss: 0.9315 - val_acc: 0.3038\n",
      "Epoch 23/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9190 - acc: 0.3901 - val_loss: 0.9556 - val_acc: 0.1897\n",
      "Epoch 24/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9187 - acc: 0.4084 - val_loss: 0.9260 - val_acc: 0.3549\n",
      "Epoch 25/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9192 - acc: 0.4025 - val_loss: 0.9332 - val_acc: 0.3799\n",
      "Epoch 26/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9187 - acc: 0.4052 - val_loss: 0.9312 - val_acc: 0.3508\n",
      "Epoch 27/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9188 - acc: 0.4249 - val_loss: 0.9263 - val_acc: 0.3228\n",
      "Epoch 28/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9185 - acc: 0.4134 - val_loss: 0.9285 - val_acc: 0.3668\n",
      "Epoch 29/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9184 - acc: 0.4157 - val_loss: 0.9270 - val_acc: 0.3288\n",
      "Epoch 30/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9184 - acc: 0.4111 - val_loss: 0.9272 - val_acc: 0.3621\n",
      "Epoch 31/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9184 - acc: 0.4011 - val_loss: 0.9260 - val_acc: 0.3347\n",
      "Epoch 32/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9182 - acc: 0.4016 - val_loss: 0.9249 - val_acc: 0.3537\n",
      "Epoch 33/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9182 - acc: 0.4029 - val_loss: 0.9270 - val_acc: 0.4102\n",
      "Epoch 34/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9182 - acc: 0.4139 - val_loss: 0.9306 - val_acc: 0.3032\n",
      "Epoch 35/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9179 - acc: 0.4198 - val_loss: 0.9256 - val_acc: 0.4441\n",
      "Epoch 36/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9179 - acc: 0.4235 - val_loss: 0.9258 - val_acc: 0.3740\n",
      "Epoch 37/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9181 - acc: 0.4057 - val_loss: 0.9258 - val_acc: 0.3829\n",
      "Epoch 38/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9178 - acc: 0.4102 - val_loss: 0.9290 - val_acc: 0.3971\n",
      "Epoch 39/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9179 - acc: 0.4203 - val_loss: 0.9292 - val_acc: 0.3044\n",
      "Epoch 40/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9177 - acc: 0.4354 - val_loss: 0.9285 - val_acc: 0.3442\n",
      "Epoch 41/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9178 - acc: 0.4299 - val_loss: 0.9261 - val_acc: 0.3615\n",
      "Epoch 42/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9176 - acc: 0.4235 - val_loss: 0.9285 - val_acc: 0.3317\n",
      "Epoch 43/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9178 - acc: 0.4349 - val_loss: 0.9301 - val_acc: 0.3811\n",
      "Epoch 44/100\n",
      "2189/2189 [==============================] - 8s 3ms/step - loss: 0.9185 - acc: 0.4175 - val_loss: 0.9299 - val_acc: 0.3157\n",
      "Epoch 45/100\n",
      "2189/2189 [==============================] - 8s 3ms/step - loss: 0.9182 - acc: 0.4116 - val_loss: 0.9316 - val_acc: 0.3276\n",
      "Epoch 46/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9175 - acc: 0.4390 - val_loss: 0.9326 - val_acc: 0.2646\n",
      "Epoch 47/100\n",
      "2189/2189 [==============================] - 8s 3ms/step - loss: 0.9174 - acc: 0.4349 - val_loss: 0.9289 - val_acc: 0.2461\n",
      "Epoch 48/100\n",
      "2189/2189 [==============================] - 8s 3ms/step - loss: 0.9175 - acc: 0.4573 - val_loss: 0.9417 - val_acc: 0.2390\n",
      "Epoch 49/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9176 - acc: 0.4564 - val_loss: 0.9256 - val_acc: 0.2729\n",
      "Epoch 50/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9175 - acc: 0.4564 - val_loss: 0.9302 - val_acc: 0.3377\n",
      "Epoch 51/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9177 - acc: 0.4545 - val_loss: 0.9259 - val_acc: 0.3835\n",
      "Epoch 52/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9175 - acc: 0.4536 - val_loss: 0.9299 - val_acc: 0.3864\n",
      "Epoch 53/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9175 - acc: 0.4450 - val_loss: 0.9293 - val_acc: 0.3936\n",
      "Epoch 54/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9176 - acc: 0.4450 - val_loss: 0.9315 - val_acc: 0.2307\n",
      "Epoch 55/100\n",
      "2189/2189 [==============================] - 8s 3ms/step - loss: 0.9174 - acc: 0.4468 - val_loss: 0.9356 - val_acc: 0.2212\n",
      "Epoch 56/100\n",
      "2189/2189 [==============================] - 8s 3ms/step - loss: 0.9173 - acc: 0.4340 - val_loss: 0.9297 - val_acc: 0.3567\n",
      "Epoch 57/100\n",
      "2189/2189 [==============================] - 8s 3ms/step - loss: 0.9173 - acc: 0.4386 - val_loss: 0.9275 - val_acc: 0.2806\n",
      "Epoch 58/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9192 - acc: 0.4285 - val_loss: 0.9342 - val_acc: 0.3989\n",
      "Epoch 59/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9194 - acc: 0.4404 - val_loss: 0.9316 - val_acc: 0.3264\n",
      "Epoch 60/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9181 - acc: 0.4450 - val_loss: 0.9281 - val_acc: 0.4501\n",
      "Epoch 61/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9176 - acc: 0.4815 - val_loss: 0.9279 - val_acc: 0.4554\n",
      "Epoch 62/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9173 - acc: 0.4847 - val_loss: 0.9256 - val_acc: 0.3728\n",
      "Epoch 63/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9172 - acc: 0.4701 - val_loss: 0.9302 - val_acc: 0.3050\n",
      "Epoch 64/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9171 - acc: 0.4774 - val_loss: 0.9279 - val_acc: 0.3151\n",
      "Epoch 65/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9172 - acc: 0.4536 - val_loss: 0.9351 - val_acc: 0.3918\n",
      "Epoch 66/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9174 - acc: 0.4596 - val_loss: 0.9281 - val_acc: 0.3508\n",
      "Epoch 67/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9172 - acc: 0.4632 - val_loss: 0.9291 - val_acc: 0.2259\n",
      "Epoch 68/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9173 - acc: 0.4532 - val_loss: 0.9288 - val_acc: 0.2735\n",
      "Epoch 69/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9173 - acc: 0.4623 - val_loss: 0.9303 - val_acc: 0.2289\n",
      "Epoch 70/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9170 - acc: 0.4760 - val_loss: 0.9284 - val_acc: 0.2414\n",
      "Epoch 71/100\n",
      "2189/2189 [==============================] - 8s 3ms/step - loss: 0.9170 - acc: 0.4792 - val_loss: 0.9276 - val_acc: 0.3359\n",
      "Epoch 72/100\n",
      "2189/2189 [==============================] - 8s 3ms/step - loss: 0.9173 - acc: 0.4710 - val_loss: 0.9295 - val_acc: 0.3341\n",
      "Epoch 73/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9172 - acc: 0.4737 - val_loss: 0.9287 - val_acc: 0.3199\n",
      "Epoch 74/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9171 - acc: 0.4746 - val_loss: 0.9266 - val_acc: 0.3633\n",
      "Epoch 75/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9170 - acc: 0.4797 - val_loss: 0.9252 - val_acc: 0.3329\n",
      "Epoch 76/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9171 - acc: 0.4929 - val_loss: 0.9249 - val_acc: 0.2848\n",
      "Epoch 77/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9169 - acc: 0.4929 - val_loss: 0.9274 - val_acc: 0.3300\n",
      "Epoch 78/100\n",
      "2189/2189 [==============================] - 8s 3ms/step - loss: 0.9170 - acc: 0.4870 - val_loss: 0.9291 - val_acc: 0.4507\n",
      "Epoch 79/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9170 - acc: 0.4925 - val_loss: 0.9327 - val_acc: 0.2628\n",
      "Epoch 80/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9172 - acc: 0.4733 - val_loss: 0.9273 - val_acc: 0.2854\n",
      "Epoch 81/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9172 - acc: 0.4915 - val_loss: 0.9269 - val_acc: 0.4037\n",
      "Epoch 82/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9171 - acc: 0.4870 - val_loss: 0.9246 - val_acc: 0.3603\n",
      "Epoch 83/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9172 - acc: 0.4984 - val_loss: 0.9263 - val_acc: 0.4061\n",
      "Epoch 84/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9170 - acc: 0.5057 - val_loss: 0.9305 - val_acc: 0.3300\n",
      "Epoch 85/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9171 - acc: 0.4984 - val_loss: 0.9257 - val_acc: 0.3859\n",
      "Epoch 86/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9171 - acc: 0.4920 - val_loss: 0.9277 - val_acc: 0.3912\n",
      "Epoch 87/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9170 - acc: 0.5016 - val_loss: 0.9250 - val_acc: 0.4090\n",
      "Epoch 88/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9171 - acc: 0.4911 - val_loss: 0.9237 - val_acc: 0.3775\n",
      "Epoch 89/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9169 - acc: 0.5057 - val_loss: 0.9259 - val_acc: 0.3918\n",
      "Epoch 90/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9171 - acc: 0.4929 - val_loss: 0.9317 - val_acc: 0.3912\n",
      "Epoch 91/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9174 - acc: 0.4687 - val_loss: 0.9298 - val_acc: 0.4489\n",
      "Epoch 92/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9169 - acc: 0.4897 - val_loss: 0.9285 - val_acc: 0.3068\n",
      "Epoch 93/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9170 - acc: 0.5075 - val_loss: 0.9283 - val_acc: 0.3930\n",
      "Epoch 94/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9170 - acc: 0.5085 - val_loss: 0.9335 - val_acc: 0.2973\n",
      "Epoch 95/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9174 - acc: 0.4957 - val_loss: 0.9295 - val_acc: 0.4078\n",
      "Epoch 96/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9172 - acc: 0.5075 - val_loss: 0.9286 - val_acc: 0.4055\n",
      "Epoch 97/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9170 - acc: 0.5057 - val_loss: 0.9265 - val_acc: 0.3549\n",
      "Epoch 98/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9169 - acc: 0.5290 - val_loss: 0.9274 - val_acc: 0.4281\n",
      "Epoch 99/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9170 - acc: 0.5126 - val_loss: 0.9293 - val_acc: 0.4049\n",
      "Epoch 100/100\n",
      "2189/2189 [==============================] - 8s 4ms/step - loss: 0.9169 - acc: 0.5176 - val_loss: 0.9274 - val_acc: 0.4501\n",
      "1682/1682 [==============================] - 2s 926us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9274189810265259, 0.45005945324472524]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('ConvNet Template 2017 {}'.format(data_input_file))\n",
    "\n",
    "i = 0\n",
    "train_idx = folds[i][0]\n",
    "test_idx = folds[i][1]\n",
    "\n",
    "X_train, y_train = X[train_idx], y[train_idx]\n",
    "X_test, y_test = X[test_idx], y[test_idx]\n",
    "X_train = X_train.squeeze()\n",
    "X_test = X_test.squeeze()\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# Create Model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(\n",
    "    BinaryConv1D(\n",
    "        kernel_size=10,\n",
    "        filters=64,\n",
    "        padding='same',\n",
    "        input_shape=(160,3),\n",
    "        kernel_lr_multiplier=10,use_bias=False))\n",
    "model.add(BatchNormalization(momentum=0.1, epsilon=0.0001))\n",
    "model.add(Activation(binary_tanh_op))\n",
    "\n",
    "\n",
    "model.add(\n",
    "    BinaryConv1D(\n",
    "        kernel_size=10, filters=128, padding='same',\n",
    "        kernel_lr_multiplier=10,use_bias=False))\n",
    "model.add(BatchNormalization(momentum=0.1, epsilon=0.0001))\n",
    "model.add(Activation(binary_tanh_op))\n",
    "\n",
    "\n",
    "model.add(\n",
    "    BinaryConv1D(\n",
    "        kernel_size=10, filters=128, padding='same',\n",
    "        kernel_lr_multiplier=10,use_bias=False))\n",
    "model.add(BatchNormalization(momentum=0.1, epsilon=0.0001))\n",
    "model.add(Activation(binary_tanh_op))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(12, use_bias=False))\n",
    "model.add(BatchNormalization(momentum=0.1, epsilon=0.0001))\n",
    "\n",
    "# Train\n",
    "adam = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "model.compile(loss=squared_hinge, optimizer=adam, metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train, batch_size=128, epochs=100, validation_data=(X_test,y_test), verbose=1 )\n",
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet Template 2017 data/LOSO/WHARF.npz\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 160, 64)           1984      \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 160, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 160, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 160, 64)           41024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 160, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 160, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 10240)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 12)                122880    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 12)                48        \n",
      "=================================================================\n",
      "Total params: 166,448\n",
      "Trainable params: 166,168\n",
      "Non-trainable params: 280\n",
      "_________________________________________________________________\n",
      "Train on 2189 samples, validate on 1682 samples\n",
      "Epoch 1/100\n",
      "2189/2189 [==============================] - 2s 1ms/step - loss: 0.9457 - acc: 0.2627 - val_loss: 0.9365 - val_acc: 0.3276\n",
      "Epoch 2/100\n",
      "2189/2189 [==============================] - 2s 747us/step - loss: 0.9254 - acc: 0.3536 - val_loss: 0.9279 - val_acc: 0.3074\n",
      "Epoch 3/100\n",
      "2189/2189 [==============================] - 2s 744us/step - loss: 0.9232 - acc: 0.3591 - val_loss: 0.9308 - val_acc: 0.2961\n",
      "Epoch 4/100\n",
      "2189/2189 [==============================] - 2s 749us/step - loss: 0.9213 - acc: 0.3476 - val_loss: 0.9285 - val_acc: 0.3317\n",
      "Epoch 5/100\n",
      "2189/2189 [==============================] - 2s 746us/step - loss: 0.9205 - acc: 0.3646 - val_loss: 0.9269 - val_acc: 0.3341\n",
      "Epoch 6/100\n",
      "2189/2189 [==============================] - 2s 733us/step - loss: 0.9197 - acc: 0.3682 - val_loss: 0.9250 - val_acc: 0.3716\n",
      "Epoch 7/100\n",
      "2189/2189 [==============================] - 2s 743us/step - loss: 0.9191 - acc: 0.3787 - val_loss: 0.9260 - val_acc: 0.3472\n",
      "Epoch 8/100\n",
      "2189/2189 [==============================] - 2s 751us/step - loss: 0.9189 - acc: 0.3824 - val_loss: 0.9300 - val_acc: 0.3835\n",
      "Epoch 9/100\n",
      "2189/2189 [==============================] - 2s 743us/step - loss: 0.9185 - acc: 0.3792 - val_loss: 0.9260 - val_acc: 0.3294\n",
      "Epoch 10/100\n",
      "2189/2189 [==============================] - 2s 752us/step - loss: 0.9188 - acc: 0.3993 - val_loss: 0.9277 - val_acc: 0.4073\n",
      "Epoch 11/100\n",
      "2189/2189 [==============================] - 2s 747us/step - loss: 0.9182 - acc: 0.4066 - val_loss: 0.9259 - val_acc: 0.4043\n",
      "Epoch 12/100\n",
      "2189/2189 [==============================] - 2s 746us/step - loss: 0.9181 - acc: 0.4148 - val_loss: 0.9251 - val_acc: 0.4096\n",
      "Epoch 13/100\n",
      "2189/2189 [==============================] - 2s 752us/step - loss: 0.9180 - acc: 0.4166 - val_loss: 0.9243 - val_acc: 0.4168\n",
      "Epoch 14/100\n",
      "2189/2189 [==============================] - 2s 738us/step - loss: 0.9179 - acc: 0.4189 - val_loss: 0.9251 - val_acc: 0.3918\n",
      "Epoch 15/100\n",
      "2189/2189 [==============================] - 2s 743us/step - loss: 0.9175 - acc: 0.4285 - val_loss: 0.9250 - val_acc: 0.4388\n",
      "Epoch 16/100\n",
      "2189/2189 [==============================] - 2s 746us/step - loss: 0.9176 - acc: 0.4317 - val_loss: 0.9245 - val_acc: 0.4191\n",
      "Epoch 17/100\n",
      "2189/2189 [==============================] - 2s 745us/step - loss: 0.9173 - acc: 0.4468 - val_loss: 0.9247 - val_acc: 0.4245\n",
      "Epoch 18/100\n",
      "2189/2189 [==============================] - 2s 748us/step - loss: 0.9173 - acc: 0.4418 - val_loss: 0.9277 - val_acc: 0.4560\n",
      "Epoch 19/100\n",
      "2189/2189 [==============================] - 2s 747us/step - loss: 0.9172 - acc: 0.4427 - val_loss: 0.9251 - val_acc: 0.4007\n",
      "Epoch 20/100\n",
      "2189/2189 [==============================] - 2s 751us/step - loss: 0.9171 - acc: 0.4509 - val_loss: 0.9255 - val_acc: 0.4233\n",
      "Epoch 21/100\n",
      "2189/2189 [==============================] - 2s 750us/step - loss: 0.9174 - acc: 0.4527 - val_loss: 0.9258 - val_acc: 0.4512\n",
      "Epoch 22/100\n",
      "2189/2189 [==============================] - 2s 743us/step - loss: 0.9171 - acc: 0.4701 - val_loss: 0.9259 - val_acc: 0.4227\n",
      "Epoch 23/100\n",
      "2189/2189 [==============================] - 2s 745us/step - loss: 0.9172 - acc: 0.4518 - val_loss: 0.9297 - val_acc: 0.4382\n",
      "Epoch 24/100\n",
      "2189/2189 [==============================] - 2s 750us/step - loss: 0.9170 - acc: 0.4664 - val_loss: 0.9244 - val_acc: 0.4780\n",
      "Epoch 25/100\n",
      "2189/2189 [==============================] - 2s 753us/step - loss: 0.9169 - acc: 0.4683 - val_loss: 0.9255 - val_acc: 0.4382\n",
      "Epoch 26/100\n",
      "2189/2189 [==============================] - 2s 755us/step - loss: 0.9170 - acc: 0.4728 - val_loss: 0.9245 - val_acc: 0.4352\n",
      "Epoch 27/100\n",
      "2189/2189 [==============================] - 2s 758us/step - loss: 0.9168 - acc: 0.4934 - val_loss: 0.9258 - val_acc: 0.4221\n",
      "Epoch 28/100\n",
      "2189/2189 [==============================] - 2s 749us/step - loss: 0.9169 - acc: 0.4938 - val_loss: 0.9250 - val_acc: 0.4287\n",
      "Epoch 29/100\n",
      "2189/2189 [==============================] - 2s 740us/step - loss: 0.9169 - acc: 0.4824 - val_loss: 0.9258 - val_acc: 0.4507\n",
      "Epoch 30/100\n",
      "2189/2189 [==============================] - 2s 746us/step - loss: 0.9169 - acc: 0.4906 - val_loss: 0.9255 - val_acc: 0.4298\n",
      "Epoch 31/100\n",
      "2189/2189 [==============================] - 2s 742us/step - loss: 0.9169 - acc: 0.5025 - val_loss: 0.9278 - val_acc: 0.4703\n",
      "Epoch 32/100\n",
      "2189/2189 [==============================] - 2s 753us/step - loss: 0.9170 - acc: 0.5062 - val_loss: 0.9256 - val_acc: 0.4477\n",
      "Epoch 33/100\n",
      "2189/2189 [==============================] - 2s 767us/step - loss: 0.9168 - acc: 0.5107 - val_loss: 0.9240 - val_acc: 0.5012\n",
      "Epoch 34/100\n",
      "2189/2189 [==============================] - 2s 750us/step - loss: 0.9168 - acc: 0.5085 - val_loss: 0.9255 - val_acc: 0.4631\n",
      "Epoch 35/100\n",
      "2189/2189 [==============================] - 2s 744us/step - loss: 0.9169 - acc: 0.4934 - val_loss: 0.9274 - val_acc: 0.4958\n",
      "Epoch 36/100\n",
      "2189/2189 [==============================] - 2s 756us/step - loss: 0.9169 - acc: 0.5034 - val_loss: 0.9246 - val_acc: 0.4370\n",
      "Epoch 37/100\n",
      "2189/2189 [==============================] - 2s 747us/step - loss: 0.9172 - acc: 0.5153 - val_loss: 0.9245 - val_acc: 0.5131\n",
      "Epoch 38/100\n",
      "2189/2189 [==============================] - 2s 750us/step - loss: 0.9170 - acc: 0.5066 - val_loss: 0.9251 - val_acc: 0.4792\n",
      "Epoch 39/100\n",
      "2189/2189 [==============================] - 2s 748us/step - loss: 0.9168 - acc: 0.5176 - val_loss: 0.9280 - val_acc: 0.4608\n",
      "Epoch 40/100\n",
      "2189/2189 [==============================] - 2s 740us/step - loss: 0.9169 - acc: 0.5299 - val_loss: 0.9251 - val_acc: 0.4917\n",
      "Epoch 41/100\n",
      "2189/2189 [==============================] - 2s 755us/step - loss: 0.9168 - acc: 0.5468 - val_loss: 0.9270 - val_acc: 0.4453\n",
      "Epoch 42/100\n",
      "2189/2189 [==============================] - 2s 754us/step - loss: 0.9167 - acc: 0.5477 - val_loss: 0.9277 - val_acc: 0.4786\n",
      "Epoch 43/100\n",
      "2189/2189 [==============================] - 2s 742us/step - loss: 0.9168 - acc: 0.5455 - val_loss: 0.9282 - val_acc: 0.4614\n",
      "Epoch 44/100\n",
      "2189/2189 [==============================] - 2s 759us/step - loss: 0.9168 - acc: 0.5345 - val_loss: 0.9248 - val_acc: 0.4851\n",
      "Epoch 45/100\n",
      "2189/2189 [==============================] - 2s 745us/step - loss: 0.9167 - acc: 0.5610 - val_loss: 0.9259 - val_acc: 0.4941\n",
      "Epoch 46/100\n",
      "2189/2189 [==============================] - 2s 752us/step - loss: 0.9167 - acc: 0.5637 - val_loss: 0.9246 - val_acc: 0.4941\n",
      "Epoch 47/100\n",
      "2189/2189 [==============================] - 2s 748us/step - loss: 0.9168 - acc: 0.5528 - val_loss: 0.9246 - val_acc: 0.4293\n",
      "Epoch 48/100\n",
      "2189/2189 [==============================] - 2s 749us/step - loss: 0.9168 - acc: 0.5646 - val_loss: 0.9243 - val_acc: 0.4839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/100\n",
      "2189/2189 [==============================] - 2s 740us/step - loss: 0.9168 - acc: 0.5683 - val_loss: 0.9240 - val_acc: 0.4935\n",
      "Epoch 50/100\n",
      "2189/2189 [==============================] - 2s 725us/step - loss: 0.9167 - acc: 0.5596 - val_loss: 0.9253 - val_acc: 0.4637\n",
      "Epoch 51/100\n",
      "2189/2189 [==============================] - 2s 742us/step - loss: 0.9168 - acc: 0.5537 - val_loss: 0.9246 - val_acc: 0.5155\n",
      "Epoch 52/100\n",
      "2189/2189 [==============================] - 2s 736us/step - loss: 0.9168 - acc: 0.5720 - val_loss: 0.9255 - val_acc: 0.4566\n",
      "Epoch 53/100\n",
      "2189/2189 [==============================] - 2s 739us/step - loss: 0.9167 - acc: 0.5861 - val_loss: 0.9257 - val_acc: 0.4887\n",
      "Epoch 54/100\n",
      "2189/2189 [==============================] - 2s 746us/step - loss: 0.9167 - acc: 0.5770 - val_loss: 0.9240 - val_acc: 0.5071\n",
      "Epoch 55/100\n",
      "2189/2189 [==============================] - 2s 741us/step - loss: 0.9168 - acc: 0.5834 - val_loss: 0.9247 - val_acc: 0.4798\n",
      "Epoch 56/100\n",
      "2189/2189 [==============================] - 2s 748us/step - loss: 0.9168 - acc: 0.5624 - val_loss: 0.9257 - val_acc: 0.5018\n",
      "Epoch 57/100\n",
      "2189/2189 [==============================] - 2s 736us/step - loss: 0.9170 - acc: 0.5605 - val_loss: 0.9293 - val_acc: 0.4524\n",
      "Epoch 58/100\n",
      "2189/2189 [==============================] - 2s 739us/step - loss: 0.9169 - acc: 0.5715 - val_loss: 0.9268 - val_acc: 0.4691\n",
      "Epoch 59/100\n",
      "2189/2189 [==============================] - 2s 734us/step - loss: 0.9168 - acc: 0.5857 - val_loss: 0.9254 - val_acc: 0.4839\n",
      "Epoch 60/100\n",
      "2189/2189 [==============================] - 2s 734us/step - loss: 0.9168 - acc: 0.5925 - val_loss: 0.9250 - val_acc: 0.5054\n",
      "Epoch 61/100\n",
      "2189/2189 [==============================] - 2s 729us/step - loss: 0.9168 - acc: 0.5843 - val_loss: 0.9244 - val_acc: 0.5386\n",
      "Epoch 62/100\n",
      "2189/2189 [==============================] - 2s 736us/step - loss: 0.9167 - acc: 0.5820 - val_loss: 0.9287 - val_acc: 0.4804\n",
      "Epoch 63/100\n",
      "2189/2189 [==============================] - 2s 745us/step - loss: 0.9167 - acc: 0.5925 - val_loss: 0.9272 - val_acc: 0.4411\n",
      "Epoch 64/100\n",
      "2189/2189 [==============================] - 2s 738us/step - loss: 0.9167 - acc: 0.5984 - val_loss: 0.9265 - val_acc: 0.4964\n",
      "Epoch 65/100\n",
      "2189/2189 [==============================] - 2s 753us/step - loss: 0.9167 - acc: 0.5811 - val_loss: 0.9258 - val_acc: 0.4988\n",
      "Epoch 66/100\n",
      "2189/2189 [==============================] - 2s 737us/step - loss: 0.9167 - acc: 0.6021 - val_loss: 0.9280 - val_acc: 0.5018\n",
      "Epoch 67/100\n",
      "2189/2189 [==============================] - 2s 744us/step - loss: 0.9167 - acc: 0.6021 - val_loss: 0.9266 - val_acc: 0.5184\n",
      "Epoch 68/100\n",
      "2189/2189 [==============================] - 2s 741us/step - loss: 0.9167 - acc: 0.6048 - val_loss: 0.9236 - val_acc: 0.5571\n",
      "Epoch 69/100\n",
      "2189/2189 [==============================] - 2s 730us/step - loss: 0.9167 - acc: 0.5952 - val_loss: 0.9266 - val_acc: 0.4875\n",
      "Epoch 70/100\n",
      "2189/2189 [==============================] - 2s 749us/step - loss: 0.9167 - acc: 0.6053 - val_loss: 0.9262 - val_acc: 0.4935\n",
      "Epoch 71/100\n",
      "2189/2189 [==============================] - 2s 740us/step - loss: 0.9167 - acc: 0.6185 - val_loss: 0.9260 - val_acc: 0.5113\n",
      "Epoch 72/100\n",
      "2189/2189 [==============================] - 2s 743us/step - loss: 0.9167 - acc: 0.5994 - val_loss: 0.9256 - val_acc: 0.5547\n",
      "Epoch 73/100\n",
      "2189/2189 [==============================] - 2s 746us/step - loss: 0.9168 - acc: 0.5898 - val_loss: 0.9253 - val_acc: 0.5285\n",
      "Epoch 74/100\n",
      "2189/2189 [==============================] - 2s 743us/step - loss: 0.9167 - acc: 0.6190 - val_loss: 0.9252 - val_acc: 0.5262\n",
      "Epoch 75/100\n",
      "2189/2189 [==============================] - 2s 743us/step - loss: 0.9167 - acc: 0.6094 - val_loss: 0.9245 - val_acc: 0.5071\n",
      "Epoch 76/100\n",
      "2189/2189 [==============================] - 2s 735us/step - loss: 0.9167 - acc: 0.6263 - val_loss: 0.9246 - val_acc: 0.5256\n",
      "Epoch 77/100\n",
      "2189/2189 [==============================] - 2s 741us/step - loss: 0.9167 - acc: 0.6048 - val_loss: 0.9259 - val_acc: 0.5327\n",
      "Epoch 78/100\n",
      "2189/2189 [==============================] - 2s 739us/step - loss: 0.9167 - acc: 0.6158 - val_loss: 0.9273 - val_acc: 0.4905\n",
      "Epoch 79/100\n",
      "2189/2189 [==============================] - 2s 733us/step - loss: 0.9167 - acc: 0.6012 - val_loss: 0.9304 - val_acc: 0.4988\n",
      "Epoch 80/100\n",
      "2189/2189 [==============================] - 2s 742us/step - loss: 0.9167 - acc: 0.6268 - val_loss: 0.9281 - val_acc: 0.4834\n",
      "Epoch 81/100\n",
      "2189/2189 [==============================] - 2s 754us/step - loss: 0.9167 - acc: 0.6217 - val_loss: 0.9261 - val_acc: 0.5321\n",
      "Epoch 82/100\n",
      "2189/2189 [==============================] - 2s 746us/step - loss: 0.9167 - acc: 0.6254 - val_loss: 0.9251 - val_acc: 0.5250\n",
      "Epoch 83/100\n",
      "2189/2189 [==============================] - 2s 756us/step - loss: 0.9167 - acc: 0.6076 - val_loss: 0.9283 - val_acc: 0.4756\n",
      "Epoch 84/100\n",
      "2189/2189 [==============================] - 2s 740us/step - loss: 0.9167 - acc: 0.6167 - val_loss: 0.9275 - val_acc: 0.5214\n",
      "Epoch 85/100\n",
      "2189/2189 [==============================] - 2s 746us/step - loss: 0.9167 - acc: 0.6204 - val_loss: 0.9259 - val_acc: 0.5321\n",
      "Epoch 86/100\n",
      "2189/2189 [==============================] - 2s 736us/step - loss: 0.9167 - acc: 0.6204 - val_loss: 0.9258 - val_acc: 0.5161\n",
      "Epoch 87/100\n",
      "2189/2189 [==============================] - 2s 753us/step - loss: 0.9167 - acc: 0.6259 - val_loss: 0.9256 - val_acc: 0.5048\n",
      "Epoch 88/100\n",
      "2189/2189 [==============================] - 2s 745us/step - loss: 0.9167 - acc: 0.6048 - val_loss: 0.9259 - val_acc: 0.4899\n",
      "Epoch 89/100\n",
      "2189/2189 [==============================] - 2s 743us/step - loss: 0.9167 - acc: 0.6185 - val_loss: 0.9243 - val_acc: 0.5517\n",
      "Epoch 90/100\n",
      "2189/2189 [==============================] - 2s 746us/step - loss: 0.9167 - acc: 0.6085 - val_loss: 0.9257 - val_acc: 0.5119\n",
      "Epoch 91/100\n",
      "2189/2189 [==============================] - 2s 746us/step - loss: 0.9167 - acc: 0.6062 - val_loss: 0.9248 - val_acc: 0.5155\n",
      "Epoch 92/100\n",
      "2189/2189 [==============================] - 2s 745us/step - loss: 0.9167 - acc: 0.6058 - val_loss: 0.9242 - val_acc: 0.4899\n",
      "Epoch 93/100\n",
      "2189/2189 [==============================] - 2s 752us/step - loss: 0.9167 - acc: 0.6076 - val_loss: 0.9251 - val_acc: 0.5523\n",
      "Epoch 94/100\n",
      "2189/2189 [==============================] - 2s 741us/step - loss: 0.9167 - acc: 0.5911 - val_loss: 0.9270 - val_acc: 0.5125\n",
      "Epoch 95/100\n",
      "2189/2189 [==============================] - 2s 760us/step - loss: 0.9167 - acc: 0.6012 - val_loss: 0.9249 - val_acc: 0.5161\n",
      "Epoch 96/100\n",
      "2189/2189 [==============================] - 2s 744us/step - loss: 0.9168 - acc: 0.6158 - val_loss: 0.9260 - val_acc: 0.4976\n",
      "Epoch 97/100\n",
      "2189/2189 [==============================] - 2s 748us/step - loss: 0.9168 - acc: 0.5560 - val_loss: 0.9264 - val_acc: 0.4400\n",
      "Epoch 98/100\n",
      "2189/2189 [==============================] - 2s 747us/step - loss: 0.9167 - acc: 0.5975 - val_loss: 0.9251 - val_acc: 0.4661\n",
      "Epoch 99/100\n",
      "2189/2189 [==============================] - 2s 740us/step - loss: 0.9172 - acc: 0.5857 - val_loss: 0.9261 - val_acc: 0.4893\n",
      "Epoch 100/100\n",
      "2189/2189 [==============================] - 2s 751us/step - loss: 0.9168 - acc: 0.5843 - val_loss: 0.9258 - val_acc: 0.4620\n",
      "1682/1682 [==============================] - 0s 201us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9258174429183624, 0.46195005946189127]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_input_file = 'data/LOSO/WHARF.npz'\n",
    "\n",
    "tmp = np.load(data_input_file)\n",
    "X = tmp['X']\n",
    "y = tmp['y']\n",
    "folds = tmp['folds']\n",
    "\n",
    "n_class = y.shape[1]\n",
    "\n",
    "avg_acc = []\n",
    "avg_recall = []\n",
    "avg_f1 = []\n",
    "y = np.argmax(y, axis=1)\n",
    "\n",
    "print('ConvNet Template 2017 {}'.format(data_input_file))\n",
    "\n",
    "i = 0\n",
    "train_idx = folds[i][0]\n",
    "test_idx = folds[i][1]\n",
    "\n",
    "X_train, y_train = X[train_idx], y[train_idx]\n",
    "X_test, y_test = X[test_idx], y[test_idx]\n",
    "X_train = X_train.squeeze()\n",
    "X_test = X_test.squeeze()\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# Create Model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(\n",
    "    Conv1D(\n",
    "        kernel_size=10,\n",
    "        filters=64,\n",
    "        padding='same',\n",
    "        input_shape=(160,3)))\n",
    "model.add(BatchNormalization(momentum=0.1, epsilon=0.0001))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(\n",
    "        kernel_size=10, filters=64, padding='same'))\n",
    "model.add(BatchNormalization(momentum=0.1, epsilon=0.0001))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(12, use_bias=False))\n",
    "model.add(BatchNormalization(momentum=0.1, epsilon=0.0001))\n",
    "\n",
    "# Train\n",
    "adam = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "model.compile(loss=squared_hinge, optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "model.fit(\n",
    "    X_train, y_train, batch_size=50, epochs=100, validation_data=(X_test,y_test))\n",
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248/248 [==============================] - 0s 191us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9168156596922106, 0.6653225806451613]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
